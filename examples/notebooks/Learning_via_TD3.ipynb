{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using TD3 to learn a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to use TD3 to learn a policy for a simple `ElectricGridEnv` environment. The environment is the same as the one used in {RL_Complex_DEMO_external_agent}. Furthermore, we introduce how to use `Wandb.jl` to log the training process. For more information on how to customize `Wandb.jl` logging, please refer to the [documentation](https://avik-pal.github.io/Wandb.jl/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Tracking run with wandb version 0.15.5\n",
      "wandb: Run data is saved locally in /data/cvikas/Projects/ElectricGrid.jl/examples/notebooks/wandb/run-20230713_171147-xytx27cf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run train with TD3\n",
      "wandb: â­ï¸ View project at https://wandb.ai/electricgrid-jl/TD3\n",
      "wandb: ðŸš€ View run at https://wandb.ai/electricgrid-jl/TD3/runs/xytx27cf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WandbLogger:\n",
       "  Project: TD3\n",
       "  Name: train with TD3}\n",
       "  Id: trdezwl4\n",
       "  Min Level: Info\n",
       "  Url: \u001b[33mhttps://wandb.ai/electricgrid-jl/TD3/runs/trdezwl4\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ElectricGrid\n",
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using StableRNGs\n",
    "using IntervalSets\n",
    "using Zygote: ignore\n",
    "using Logging\n",
    "using Wandb\n",
    "\n",
    "td3_src_dir = joinpath(dirname(pathof(ElectricGrid)))\n",
    "include(td3_src_dir * \"/agent_td3.jl\")\n",
    "\n",
    "# make sure to have a wandb account and be logged in\n",
    "# https://docs.wandb.ai/quickstart\n",
    "logger = WandbLogger(\n",
    "    # Provide a project name and an entity name\n",
    "    project=\"TD3\",\n",
    "    # optionally provide a team name if it is created\n",
    "    entity=\"electricgrid-jl\",\n",
    "    # optionally provide a run name\n",
    "    name=\"train with TD3\",\n",
    "    # optionally provide a config\n",
    "    config=Dict(\n",
    "        \"lr\" => 3e-5,\n",
    "        ),\n",
    ")\n",
    "\n",
    "global_logger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same environment configuration \n",
    "CM = [ \n",
    "    0. 0. 1.\n",
    "    0. 0. 2.\n",
    "   -1. -2. 0.\n",
    "]\n",
    "\n",
    "R_load, L_load, _, _ = ParallelLoadImpedance(50e3, 0.95, 230)\n",
    "\n",
    "parameters = Dict{Any, Any}(\n",
    "                    \"source\" => Any[\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"control_type\" => \"RL\",\n",
    "                                        \"mode\" => \"my_agent\",\n",
    "                                        \"fltr\" => \"L\",\n",
    "                                        #\"L1\" => 0.0008,\n",
    "                                        ),\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"fltr\" => \"LC\",\n",
    "                                        \"control_type\" => \"classic\",\n",
    "                                        \"mode\" => \"Droop\",),\n",
    "                                    ],\n",
    "                    \"grid\" => Dict{Any, Any}(\n",
    "                        \"phase\" => 3,\n",
    "                        \"ramp_end\" => 0.04,)\n",
    "    )\n",
    "\n",
    "\n",
    "function reference(t)\n",
    "    if t < 0.04\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    end\n",
    "\n",
    "    Î¸ = 2*pi*50*t\n",
    "    Î¸ph = [Î¸; Î¸ - 120Ï€/180; Î¸ + 120Ï€/180]\n",
    "    return +10 * cos.(Î¸ph) \n",
    "end\n",
    "\n",
    "\n",
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_agent\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function reward_function(env, name = nothing)\n",
    "    if name == \"classic\"\n",
    "        return 0        \n",
    "    else\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1_a\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source1_i_L1_b\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source1_i_L1_c\", env.state_ids)]\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "            return r \n",
    "        end\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "env = ElectricGridEnv(\n",
    "    #CM =  CM,\n",
    "    parameters = parameters,\n",
    "    t_end = 1,\n",
    "    reward_function = reward_function,\n",
    "    featurize = featurize_ddpg,\n",
    "    action_delay = 0,\n",
    "    verbosity = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have slightly modified the original TD3 implementation in Julia to make it compatible with the `ElectricGrid.jl` framework.  \n",
    "\n",
    "Details about the TD3 algorithm can be found in the paper [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) and the original implementation in Julia can be found [here](https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/2e1de3e5b6b8224f50b3d11bba7e1d2d72c6ef7c/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/td3.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "â”œâ”€ policy => typename(TD3Policy)\n",
       "â”‚  â”œâ”€ behavior_actor => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”œâ”€ model => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 32Ã—9 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 32-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 32Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 32-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚           â”œâ”€ weight => 3Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚           â”œâ”€ bias => 3-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(tanh))\n",
       "â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚     â”œâ”€ eta => 3.0e-5\n",
       "â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”œâ”€ behavior_critic => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”œâ”€ model => typename(TD3Critic)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ critic_1 => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 64Ã—12 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 64Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 1Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â””â”€ critic_2 => typename(Chain)\n",
       "â”‚  â”‚  â”‚     â””â”€ layers\n",
       "â”‚  â”‚  â”‚        â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 64Ã—12 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚        â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 64Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚        â””â”€ 3\n",
       "â”‚  â”‚  â”‚           â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚              â”œâ”€ weight => 1Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚              â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚              â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚     â”œâ”€ eta => 3.0e-5\n",
       "â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”œâ”€ target_actor => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”œâ”€ model => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 32Ã—9 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 32-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 32Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 32-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚           â”œâ”€ weight => 3Ã—32 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚           â”œâ”€ bias => 3-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(tanh))\n",
       "â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚     â”œâ”€ eta => 3.0e-5\n",
       "â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”œâ”€ target_critic => typename(NeuralNetworkApproximator)\n",
       "â”‚  â”‚  â”œâ”€ model => typename(TD3Critic)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ critic_1 => typename(Chain)\n",
       "â”‚  â”‚  â”‚  â”‚  â””â”€ layers\n",
       "â”‚  â”‚  â”‚  â”‚     â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 64Ã—12 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚  â”‚     â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ weight => 64Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚     â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚  â”‚     â””â”€ 3\n",
       "â”‚  â”‚  â”‚  â”‚        â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚  â”‚           â”œâ”€ weight => 1Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚           â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚  â”‚           â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â”‚  â””â”€ critic_2 => typename(Chain)\n",
       "â”‚  â”‚  â”‚     â””â”€ layers\n",
       "â”‚  â”‚  â”‚        â”œâ”€ 1\n",
       "â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 64Ã—12 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚        â”œâ”€ 2\n",
       "â”‚  â”‚  â”‚        â”‚  â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ weight => 64Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â”œâ”€ bias => 64-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚        â”‚     â””â”€ Ïƒ => typename(typeof(relu))\n",
       "â”‚  â”‚  â”‚        â””â”€ 3\n",
       "â”‚  â”‚  â”‚           â””â”€ typename(Dense)\n",
       "â”‚  â”‚  â”‚              â”œâ”€ weight => 1Ã—64 Matrix{Float32}\n",
       "â”‚  â”‚  â”‚              â”œâ”€ bias => 1-element Vector{Float32}\n",
       "â”‚  â”‚  â”‚              â””â”€ Ïƒ => typename(typeof(identity))\n",
       "â”‚  â”‚  â””â”€ optimizer => typename(ADAM)\n",
       "â”‚  â”‚     â”œâ”€ eta => 3.0e-5\n",
       "â”‚  â”‚     â”œâ”€ beta\n",
       "â”‚  â”‚     â”‚  â”œâ”€ 1\n",
       "â”‚  â”‚     â”‚  â”‚  â””â”€ 0.9\n",
       "â”‚  â”‚     â”‚  â””â”€ 2\n",
       "â”‚  â”‚     â”‚     â””â”€ 0.999\n",
       "â”‚  â”‚     â”œâ”€ epsilon => 1.0e-8\n",
       "â”‚  â”‚     â””â”€ state => typename(IdDict)\n",
       "â”‚  â”œâ”€ Î³ => 0.99\n",
       "â”‚  â”œâ”€ Ï => 0.995\n",
       "â”‚  â”œâ”€ batch_size => 64\n",
       "â”‚  â”œâ”€ start_steps => 10\n",
       "â”‚  â”œâ”€ start_policy => typename(RandomPolicy)\n",
       "â”‚  â”‚  â”œâ”€ action_space => typename(Interval)\n",
       "â”‚  â”‚  â”‚  â”œâ”€ left => -1.0\n",
       "â”‚  â”‚  â”‚  â””â”€ right => 1.0\n",
       "â”‚  â”‚  â””â”€ rng => typename(StableRNGs.LehmerRNG)\n",
       "â”‚  â”œâ”€ update_after => 10\n",
       "â”‚  â”œâ”€ update_freq => 1\n",
       "â”‚  â”œâ”€ policy_freq => 2\n",
       "â”‚  â”œâ”€ target_act_limit => 1.0\n",
       "â”‚  â”œâ”€ target_act_noise => 0.1\n",
       "â”‚  â”œâ”€ act_limit => 1.0\n",
       "â”‚  â”œâ”€ act_noise => 0.05\n",
       "â”‚  â”œâ”€ update_step => 0\n",
       "â”‚  â”œâ”€ rng => typename(StableRNGs.LehmerRNG)\n",
       "â”‚  â”œâ”€ replay_counter => 1\n",
       "â”‚  â”œâ”€ actor_loss => 0.0\n",
       "â”‚  â””â”€ critic_loss => 0.0\n",
       "â””â”€ trajectory => typename(Trajectory)\n",
       "   â””â”€ traces => typename(NamedTuple)\n",
       "      â”œâ”€ state => 9Ã—0 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "      â”œâ”€ action => 3Ã—0 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "      â”œâ”€ reward => 0-element CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}\n",
       "      â””â”€ terminal => 0-element CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = StableRNG(0)\n",
    "init = glorot_uniform(rng)\n",
    "\n",
    "# specify number of states and actions to be controlled by the agent\n",
    "ns = length(ElectricGrid.state(env, \"my_agent\"))\n",
    "na = length(env.agent_dict[\"my_agent\"][\"action_ids\"])\n",
    "\n",
    "CreateActor() = Chain(\n",
    "    Dense(ns, 32, relu; init = init),\n",
    "    Dense(32, 32, relu; init = init),\n",
    "    Dense(32, na, tanh; init = init)\n",
    ")\n",
    "\n",
    "CreateCriticModel() = Chain(\n",
    "    Dense(ns + na, 64, relu; init = init),\n",
    "    Dense(64, 64, relu; init = init),\n",
    "    Dense(64, 1; init = init)\n",
    ")\n",
    "\n",
    "# struct TD3Critic\n",
    "#     critic_1::Flux.Chain\n",
    "#     critic_2::Flux.Chain\n",
    "# end\n",
    "\n",
    "\n",
    "# Flux.@functor TD3Critic\n",
    "\n",
    "# create twin critic models\n",
    "CreateCritic() = TD3Critic(\n",
    "    CreateCriticModel(),\n",
    "    CreateCriticModel(),\n",
    ")\n",
    "\n",
    "# learning_rate = logger.config[\"lr\"]\n",
    "learning_rate = 3e-5\n",
    "\n",
    "TD3_agent = Agent(\n",
    "    policy = TD3Policy(\n",
    "        behavior_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        behavior_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        Î³ = 0.99f0,\n",
    "        Ï = 0.995f0,\n",
    "        batch_size = 64,\n",
    "        start_steps = 10,\n",
    "        # start_steps = -1,\n",
    "        start_policy = RandomPolicy(-1.0..1.0; rng = rng),\n",
    "        update_after = 10,\n",
    "        update_freq = 1,\n",
    "        policy_freq = 2,\n",
    "        target_act_limit = 1.0,\n",
    "        target_act_noise = 0.1,\n",
    "        act_limit = 1.0,\n",
    "        act_noise = 0.05,\n",
    "        rng = rng,\n",
    "    ),\n",
    "\n",
    "    trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 10_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Float32 => (na, ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_agent = Dict(\"my_agent\" => TD3_agent)\n",
    "\n",
    "controllers = SetupAgents(env, td3_agent)\n",
    "\n",
    "Learn(\n",
    "    controllers,\n",
    "    env, \n",
    "    num_episodes = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: \\ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: ðŸš€ View run train with TD3 at: https://wandb.ai/electricgrid-jl/TD3/runs/xytx27cf\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230713_171147-xytx27cf/logs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Python: None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# close wandb session\n",
    "close(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
