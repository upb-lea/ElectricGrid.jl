{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using TD3 to learn a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to use TD3 to learn a policy for a simple `ElectricGridEnv` environment. The environment is the same as the one used in {RL_Complex_DEMO_external_agent}. Furthermore, we introduce how to use `Wandb.jl` to log the training process. For more information on how to customize `Wandb.jl` logging, please refer to the [documentation](https://avik-pal.github.io/Wandb.jl/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Tracking run with wandb version 0.15.5\n",
      "wandb: Run data is saved locally in /data/cvikas/Projects/ElectricGrid.jl/examples/notebooks/wandb/run-20230713_171147-xytx27cf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run train with TD3\n",
      "wandb: ⭐️ View project at https://wandb.ai/electricgrid-jl/TD3\n",
      "wandb: 🚀 View run at https://wandb.ai/electricgrid-jl/TD3/runs/xytx27cf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WandbLogger:\n",
       "  Project: TD3\n",
       "  Name: train with TD3}\n",
       "  Id: trdezwl4\n",
       "  Min Level: Info\n",
       "  Url: \u001b[33mhttps://wandb.ai/electricgrid-jl/TD3/runs/trdezwl4\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ElectricGrid\n",
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using StableRNGs\n",
    "using IntervalSets\n",
    "using Zygote: ignore\n",
    "using Logging\n",
    "using Wandb\n",
    "\n",
    "td3_src_dir = joinpath(dirname(pathof(ElectricGrid)))\n",
    "include(td3_src_dir * \"/agent_td3.jl\")\n",
    "\n",
    "# make sure to have a wandb account and be logged in\n",
    "# https://docs.wandb.ai/quickstart\n",
    "logger = WandbLogger(\n",
    "    # Provide a project name and an entity name\n",
    "    project=\"TD3\",\n",
    "    # optionally provide a team name if it is created\n",
    "    entity=\"electricgrid-jl\",\n",
    "    # optionally provide a run name\n",
    "    name=\"train with TD3\",\n",
    "    # optionally provide a config\n",
    "    config=Dict(\n",
    "        \"lr\" => 3e-5,\n",
    "        ),\n",
    ")\n",
    "\n",
    "global_logger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same environment configuration \n",
    "CM = [ \n",
    "    0. 0. 1.\n",
    "    0. 0. 2.\n",
    "   -1. -2. 0.\n",
    "]\n",
    "\n",
    "R_load, L_load, _, _ = ParallelLoadImpedance(50e3, 0.95, 230)\n",
    "\n",
    "parameters = Dict{Any, Any}(\n",
    "                    \"source\" => Any[\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"control_type\" => \"RL\",\n",
    "                                        \"mode\" => \"my_agent\",\n",
    "                                        \"fltr\" => \"L\",\n",
    "                                        #\"L1\" => 0.0008,\n",
    "                                        ),\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"fltr\" => \"LC\",\n",
    "                                        \"control_type\" => \"classic\",\n",
    "                                        \"mode\" => \"Droop\",),\n",
    "                                    ],\n",
    "                    \"grid\" => Dict{Any, Any}(\n",
    "                        \"phase\" => 3,\n",
    "                        \"ramp_end\" => 0.04,)\n",
    "    )\n",
    "\n",
    "\n",
    "function reference(t)\n",
    "    if t < 0.04\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    end\n",
    "\n",
    "    θ = 2*pi*50*t\n",
    "    θph = [θ; θ - 120π/180; θ + 120π/180]\n",
    "    return +10 * cos.(θph) \n",
    "end\n",
    "\n",
    "\n",
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_agent\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function reward_function(env, name = nothing)\n",
    "    if name == \"classic\"\n",
    "        return 0        \n",
    "    else\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1_a\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source1_i_L1_b\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source1_i_L1_c\", env.state_ids)]\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "            return r \n",
    "        end\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "env = ElectricGridEnv(\n",
    "    #CM =  CM,\n",
    "    parameters = parameters,\n",
    "    t_end = 1,\n",
    "    reward_function = reward_function,\n",
    "    featurize = featurize_ddpg,\n",
    "    action_delay = 0,\n",
    "    verbosity = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have slightly modified the original TD3 implementation in Julia to make it compatible with the `ElectricGrid.jl` framework.  \n",
    "\n",
    "Details about the TD3 algorithm can be found in the paper [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) and the original implementation in Julia can be found [here](https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/2e1de3e5b6b8224f50b3d11bba7e1d2d72c6ef7c/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/td3.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "├─ policy => typename(TD3Policy)\n",
       "│  ├─ behavior_actor => typename(NeuralNetworkApproximator)\n",
       "│  │  ├─ model => typename(Chain)\n",
       "│  │  │  └─ layers\n",
       "│  │  │     ├─ 1\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 32×9 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 32-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     ├─ 2\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 32×32 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 32-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     └─ 3\n",
       "│  │  │        └─ typename(Dense)\n",
       "│  │  │           ├─ weight => 3×32 Matrix{Float32}\n",
       "│  │  │           ├─ bias => 3-element Vector{Float32}\n",
       "│  │  │           └─ σ => typename(typeof(tanh))\n",
       "│  │  └─ optimizer => typename(ADAM)\n",
       "│  │     ├─ eta => 3.0e-5\n",
       "│  │     ├─ beta\n",
       "│  │     │  ├─ 1\n",
       "│  │     │  │  └─ 0.9\n",
       "│  │     │  └─ 2\n",
       "│  │     │     └─ 0.999\n",
       "│  │     ├─ epsilon => 1.0e-8\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ behavior_critic => typename(NeuralNetworkApproximator)\n",
       "│  │  ├─ model => typename(TD3Critic)\n",
       "│  │  │  ├─ critic_1 => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×12 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 3\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ critic_2 => typename(Chain)\n",
       "│  │  │     └─ layers\n",
       "│  │  │        ├─ 1\n",
       "│  │  │        │  └─ typename(Dense)\n",
       "│  │  │        │     ├─ weight => 64×12 Matrix{Float32}\n",
       "│  │  │        │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │        │     └─ σ => typename(typeof(relu))\n",
       "│  │  │        ├─ 2\n",
       "│  │  │        │  └─ typename(Dense)\n",
       "│  │  │        │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │        │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │        │     └─ σ => typename(typeof(relu))\n",
       "│  │  │        └─ 3\n",
       "│  │  │           └─ typename(Dense)\n",
       "│  │  │              ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │              ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │              └─ σ => typename(typeof(identity))\n",
       "│  │  └─ optimizer => typename(ADAM)\n",
       "│  │     ├─ eta => 3.0e-5\n",
       "│  │     ├─ beta\n",
       "│  │     │  ├─ 1\n",
       "│  │     │  │  └─ 0.9\n",
       "│  │     │  └─ 2\n",
       "│  │     │     └─ 0.999\n",
       "│  │     ├─ epsilon => 1.0e-8\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ target_actor => typename(NeuralNetworkApproximator)\n",
       "│  │  ├─ model => typename(Chain)\n",
       "│  │  │  └─ layers\n",
       "│  │  │     ├─ 1\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 32×9 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 32-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     ├─ 2\n",
       "│  │  │     │  └─ typename(Dense)\n",
       "│  │  │     │     ├─ weight => 32×32 Matrix{Float32}\n",
       "│  │  │     │     ├─ bias => 32-element Vector{Float32}\n",
       "│  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │     └─ 3\n",
       "│  │  │        └─ typename(Dense)\n",
       "│  │  │           ├─ weight => 3×32 Matrix{Float32}\n",
       "│  │  │           ├─ bias => 3-element Vector{Float32}\n",
       "│  │  │           └─ σ => typename(typeof(tanh))\n",
       "│  │  └─ optimizer => typename(ADAM)\n",
       "│  │     ├─ eta => 3.0e-5\n",
       "│  │     ├─ beta\n",
       "│  │     │  ├─ 1\n",
       "│  │     │  │  └─ 0.9\n",
       "│  │     │  └─ 2\n",
       "│  │     │     └─ 0.999\n",
       "│  │     ├─ epsilon => 1.0e-8\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ target_critic => typename(NeuralNetworkApproximator)\n",
       "│  │  ├─ model => typename(TD3Critic)\n",
       "│  │  │  ├─ critic_1 => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×12 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │  │     │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 3\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │  │           ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ critic_2 => typename(Chain)\n",
       "│  │  │     └─ layers\n",
       "│  │  │        ├─ 1\n",
       "│  │  │        │  └─ typename(Dense)\n",
       "│  │  │        │     ├─ weight => 64×12 Matrix{Float32}\n",
       "│  │  │        │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │        │     └─ σ => typename(typeof(relu))\n",
       "│  │  │        ├─ 2\n",
       "│  │  │        │  └─ typename(Dense)\n",
       "│  │  │        │     ├─ weight => 64×64 Matrix{Float32}\n",
       "│  │  │        │     ├─ bias => 64-element Vector{Float32}\n",
       "│  │  │        │     └─ σ => typename(typeof(relu))\n",
       "│  │  │        └─ 3\n",
       "│  │  │           └─ typename(Dense)\n",
       "│  │  │              ├─ weight => 1×64 Matrix{Float32}\n",
       "│  │  │              ├─ bias => 1-element Vector{Float32}\n",
       "│  │  │              └─ σ => typename(typeof(identity))\n",
       "│  │  └─ optimizer => typename(ADAM)\n",
       "│  │     ├─ eta => 3.0e-5\n",
       "│  │     ├─ beta\n",
       "│  │     │  ├─ 1\n",
       "│  │     │  │  └─ 0.9\n",
       "│  │     │  └─ 2\n",
       "│  │     │     └─ 0.999\n",
       "│  │     ├─ epsilon => 1.0e-8\n",
       "│  │     └─ state => typename(IdDict)\n",
       "│  ├─ γ => 0.99\n",
       "│  ├─ ρ => 0.995\n",
       "│  ├─ batch_size => 64\n",
       "│  ├─ start_steps => 10\n",
       "│  ├─ start_policy => typename(RandomPolicy)\n",
       "│  │  ├─ action_space => typename(Interval)\n",
       "│  │  │  ├─ left => -1.0\n",
       "│  │  │  └─ right => 1.0\n",
       "│  │  └─ rng => typename(StableRNGs.LehmerRNG)\n",
       "│  ├─ update_after => 10\n",
       "│  ├─ update_freq => 1\n",
       "│  ├─ policy_freq => 2\n",
       "│  ├─ target_act_limit => 1.0\n",
       "│  ├─ target_act_noise => 0.1\n",
       "│  ├─ act_limit => 1.0\n",
       "│  ├─ act_noise => 0.05\n",
       "│  ├─ update_step => 0\n",
       "│  ├─ rng => typename(StableRNGs.LehmerRNG)\n",
       "│  ├─ replay_counter => 1\n",
       "│  ├─ actor_loss => 0.0\n",
       "│  └─ critic_loss => 0.0\n",
       "└─ trajectory => typename(Trajectory)\n",
       "   └─ traces => typename(NamedTuple)\n",
       "      ├─ state => 9×0 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "      ├─ action => 3×0 CircularArrayBuffers.CircularArrayBuffer{Float32, 2, Matrix{Float32}}\n",
       "      ├─ reward => 0-element CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}\n",
       "      └─ terminal => 0-element CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = StableRNG(0)\n",
    "init = glorot_uniform(rng)\n",
    "\n",
    "# specify number of states and actions to be controlled by the agent\n",
    "ns = length(ElectricGrid.state(env, \"my_agent\"))\n",
    "na = length(env.agent_dict[\"my_agent\"][\"action_ids\"])\n",
    "\n",
    "CreateActor() = Chain(\n",
    "    Dense(ns, 32, relu; init = init),\n",
    "    Dense(32, 32, relu; init = init),\n",
    "    Dense(32, na, tanh; init = init)\n",
    ")\n",
    "\n",
    "CreateCriticModel() = Chain(\n",
    "    Dense(ns + na, 64, relu; init = init),\n",
    "    Dense(64, 64, relu; init = init),\n",
    "    Dense(64, 1; init = init)\n",
    ")\n",
    "\n",
    "# struct TD3Critic\n",
    "#     critic_1::Flux.Chain\n",
    "#     critic_2::Flux.Chain\n",
    "# end\n",
    "\n",
    "\n",
    "# Flux.@functor TD3Critic\n",
    "\n",
    "# create twin critic models\n",
    "CreateCritic() = TD3Critic(\n",
    "    CreateCriticModel(),\n",
    "    CreateCriticModel(),\n",
    ")\n",
    "\n",
    "# learning_rate = logger.config[\"lr\"]\n",
    "learning_rate = 3e-5\n",
    "\n",
    "TD3_agent = Agent(\n",
    "    policy = TD3Policy(\n",
    "        behavior_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        behavior_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        γ = 0.99f0,\n",
    "        ρ = 0.995f0,\n",
    "        batch_size = 64,\n",
    "        start_steps = 10,\n",
    "        # start_steps = -1,\n",
    "        start_policy = RandomPolicy(-1.0..1.0; rng = rng),\n",
    "        update_after = 10,\n",
    "        update_freq = 1,\n",
    "        policy_freq = 2,\n",
    "        target_act_limit = 1.0,\n",
    "        target_act_noise = 0.1,\n",
    "        act_limit = 1.0,\n",
    "        act_noise = 0.05,\n",
    "        rng = rng,\n",
    "    ),\n",
    "\n",
    "    trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 10_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Float32 => (na, ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_agent = Dict(\"my_agent\" => TD3_agent)\n",
    "\n",
    "controllers = SetupAgents(env, td3_agent)\n",
    "\n",
    "Learn(\n",
    "    controllers,\n",
    "    env, \n",
    "    num_episodes = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: \\ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: | 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: 🚀 View run train with TD3 at: https://wandb.ai/electricgrid-jl/TD3/runs/xytx27cf\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230713_171147-xytx27cf/logs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Python: None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# close wandb session\n",
    "close(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
