{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2888dffe",
   "metadata": {},
   "source": [
    "## Train an RL agent\n",
    "This notebook will focus the following topics:\n",
    " - training an RL agent,\n",
    " - define reward function,\n",
    " - define featureize function.\n",
    "\n",
    "In this notebook a reinforcement learning agent is trained to control the current flowing through an inductor.\n",
    "It will be shown for an easy case how the agent can learn and be applied to an electrical power grid simulated with de Dare package.\n",
    "\n",
    "The use case is shown in the figure below.\n",
    "This environment consists of a single phase electrical power grid with 1 source and 1 load connected via a cable.\n",
    "\n",
    "![](figures/RL_single_agent.png \"\")\n",
    "\n",
    "First the environment is defined in the configuration shown in the figure. \n",
    "For more information how to setup an environment see `Env_Create_DEMO.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e01153",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Dare\n",
    "using ReinforcementLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f3322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 3 entries:\n",
       "  \"source\" => Any[Dict{Any, Any}(\"control_type\"=>\"RL\", \"mode\"=>\"user_def\", \"flt…\n",
       "  \"load\"   => Any[Dict{Any, Any}(\"v_limit\"=>10000.0, \"i_limit\"=>10000.0, \"L\"=>I…\n",
       "  \"grid\"   => Dict{Any, Any}(\"phase\"=>1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate passive load for wanted setting / power rating\n",
    "R_load, L_load, X, Z = Parallel_Load_Impedance(100e3, 1, 230)\n",
    "\n",
    "# define grid using CM\n",
    "CM = [0. 1.\n",
    "    -1. 0.]\n",
    "\n",
    "# Set parameters accoring graphic above\n",
    "parameters = Dict{Any, Any}(\n",
    "    \"source\" => Any[\n",
    "                    Dict{Any, Any}(\"pwr\" => 200e3, \"control_type\" => \"RL\", \"mode\" => \"user_def\", \"fltr\" => \"L\"),\n",
    "                    ],\n",
    "    \"load\"   => Any[\n",
    "                    Dict{Any, Any}(\"impedance\" => \"RL\", \"R\" => R_load, \"L\" => L_load,\"v_limit\"=>1e4, \"i_limit\"=>1e4),\n",
    "                    ],\n",
    "    \"grid\" => Dict{Any, Any}(\"phase\" => 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f039119e",
   "metadata": {},
   "source": [
    "To teach the agent that it should control the current it need on the one hand the information about which value the current shoud be (reference value) (->`featurize`) and how good the current state is which was reached using the chosen action (-> `reward`).\n",
    "\n",
    "Therefore, the reference value has to be defined. \n",
    "Here we will use a constant value to keep the example simple.\n",
    "But since the the `reference(t)` function take the simulation time as argument, more complex, time dependent signals could be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf3c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function reference(t)\n",
    "    return [1]\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55f9ce89",
   "metadata": {},
   "source": [
    "Then the `featurize()` function is defined. \n",
    "It has to jobs:\n",
    "1. Add the reference value based on the control target which should be learned:\n",
    "Here the signal generated by the `reference` function is added to the states given to the agent. This is neccessary for the agent to learn in this case that the reward is maximized if the measured current fits the the reference value.\n",
    "These reference value has to be normalized in an appropirate way that it fits to the range of the normalized states.\n",
    "\n",
    "2. Hand over the states to the agent which should be known to the agent:\n",
    "The environment can constits of more states then should be known to te agent. Reasons for example can be like shown in this examples, that the agent is supplying a load which is ,e.g., 1 km away from the source the agent controls. \n",
    "In that case it is common that the agent has no knowlegde about state of the load since to communication and measurements exchange is assumed between the source and the load.\n",
    "Anonther example can be that the electrical power grid consits of more sources and loads. The other sources are controlled by other agents or classic controllers. In that case, typically every controller / agent has knowlegde of the states of the source it controls but not about the states another agent/controller controls.\n",
    "\n",
    "Both functionalities are implemented in the following featurize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23420f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featurize (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function featurize(x0 = nothing, t0 = nothing; env = nothing, name = nothing)\n",
    "    if !isnothing(name)\n",
    "        state = env.state\n",
    "        if name == \"agent\"\n",
    "            state = state[findall(x -> x in env.state_ids_RL, env.state_ids)]\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "            state = vcat(state, reference(env.t)/norm_ref)\n",
    "        end\n",
    "    elseif isnothing(env)\n",
    "        return vcat(x0, zeros(size(reference(t0))))\n",
    "    else\n",
    "        return env.state\n",
    "    end\n",
    "    return state\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba45180b",
   "metadata": {},
   "source": [
    "Before defining the environment, the `reward()` function has to be defined to give a feedback to the agent how good the chose action was.\n",
    "First, the state to be controlled is taken from the current environment states.\n",
    "Since the states are normalized by the limits the electrical components can handle, a value greater then `1` means that the state limit is exceeded typically leading to a ssystem crash.\n",
    "\n",
    "Therefore, first it is checked if the measured state is greate then `1`. In that case a punishment is returned which , here, is chosen to `r = -1`.\n",
    "In the case that the controlled state is within the valide state space, the reward is caculated based on the error between the wanted reference value and the measured state value. \n",
    "If these values are the same, meaning the agent perfectly fullfills the control task, a reward of `r = 1` is returned to the agent. ( -> r $\\in$ [-1, 1]).\n",
    "If the measured value differs from the reference, the error - based on the root-mean square error in this example - is substracted from the maximal reward: `r = 1 - MRE`:\n",
    "\n",
    "$r = 1 - \\sqrt{\\frac{|i_\\mathrm{L,ref} - i_\\mathrm{L1}|}{2}}$\n",
    "\n",
    "To keep the reward in the wanted range, the current difference is devided by 2. (E.g., in worst case, if a reference value equal to the corresponding current limit is chosen $i_\\mathrm{L,ref} = i_\\mathrm{lim}$ and the measured current is the negative current limit $i_\\mathrm{L1} = -i_\\mathrm{lim}$ more the 1 would be substracted without / 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5137a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function reward_function(env, name = nothing)\n",
    "\n",
    "    index_1 = findfirst(x -> x == \"source1_i_L1\", env.state_ids)\n",
    "    state_to_control = [env.state[index_1]]\n",
    "\n",
    "    if any(abs.(u).>1)\n",
    "        return -1\n",
    "    else\n",
    "\n",
    "        refs = reference(env.t)\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "        r = 1-((abs.(refs/norm_ref - state_to_control)/2).^0.5)\n",
    "        return r \n",
    "    end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70bb0752",
   "metadata": {},
   "source": [
    "Then, the defined parameters, featurize and reward functions are used to create an envorinment consisting of the electircal power grid. To keep the first learning example simple the action given to the env is internally not delayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d1d66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Unable to fix variable to NaN",
     "output_type": "error",
     "traceback": [
      "Unable to fix variable to NaN\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] fix(variable::JuMP.VariableRef, value::Float64; force::Bool)\n",
      "   @ JuMP ~/.julia/packages/JuMP/yYfHy/src/variables.jl:778\n",
      " [3] fix(variable::JuMP.VariableRef, value::Float64)\n",
      "   @ JuMP ~/.julia/packages/JuMP/yYfHy/src/variables.jl:776\n",
      " [4] layout_cabels(CM::Matrix{Float64}, num_source::Int64, num_load::Int64, parameters::Dict{Any, Any}, verbosity::Int64)\n",
      "   @ Dare ~/Dokumente/GIT/dare/src/Power_System_Theory.jl:810\n",
      " [5] check_parameters(parameters::Dict{Any, Any}, num_sources::Int64, num_loads::Int64, num_connections::Int64, CM::Matrix{Float64}, ts::Float64, verbosity::Int64)\n",
      "   @ Dare ~/Dokumente/GIT/dare/src/nodeconstructor.jl:805\n",
      " [6] NodeConstructor(; num_sources::Int64, num_loads::Int64, CM::Matrix{Float64}, parameters::Dict{Any, Any}, S2S_p::Float64, S2L_p::Float64, L2L_p::Float64, ts::Float64, verbosity::Int64)\n",
      "   @ Dare ~/Dokumente/GIT/dare/src/nodeconstructor.jl:70\n",
      " [7] SimEnv(; maxsteps::Int64, ts::Float64, action_space::Nothing, state_space::Nothing, prepare_action::Nothing, featurize::Nothing, reward_function::Nothing, CM::Matrix{Float64}, num_sources::Nothing, num_loads::Nothing, parameters::Dict{Any, Any}, x0::Nothing, t0::Float64, state_ids::Nothing, convert_state_to_cpu::Bool, use_gpu::Bool, reward::Nothing, action::Nothing, action_ids::Nothing, action_delay::Int64, t_end::Float64, verbosity::Int64, agent_dict::Nothing)\n",
      "   @ Dare ~/Dokumente/GIT/dare/src/env.jl:150\n",
      " [8] top-level scope\n",
      "   @ ~/Dokumente/GIT/dare/examples/notebooks/RL_Single_Agent_DEMO.ipynb:1"
     ]
    }
   ],
   "source": [
    "env = SimEnv(\n",
    "    CM = CM, \n",
    "    parameters = parameters, \n",
    "    t_end = 0.1, \n",
    "    #featurize = featurize, \n",
    "    #reward_function = reward_function, \n",
    "    action_delay = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475ded2d",
   "metadata": {},
   "source": [
    "In this example a `Deep Deterministic Policy Gradient` agent (https://arxiv.org/abs/1509.02971, https://spinningup.openai.com/en/latest/algorithms/ddpg.html) is chosen which can learn a control task on continous state and action space.\n",
    "It is configured inside the `setup_agents()` function which takes the control types defined in the parameter dict and hands the correct indices to the corrensponding controllers / agents.\n",
    "\n",
    "TODO: Give agent(s) from the outside, automize indices used in reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1089d694",
   "metadata": {},
   "source": [
    "multi_agent = setup_agents(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f0fbaa2",
   "metadata": {},
   "source": [
    "The `setup_agent` function returns a multi_agent which is an instance of the dare package `MultiAgentGridController` which contains the different agents and classic controllers and maps their actions to the corresponding sources defined by the indices in `setup_agent`.\n",
    "\n",
    "The multi_agent inthis examples consits only of one RL agent and can be trained usin the `learn()` function to train 20 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ff2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(multi_agent, env, num_episodes = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56598c5",
   "metadata": {},
   "source": [
    "Plot a testcase using simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states_to_plot = [\"source1_v_C_filt_a\", \"source1_v_C_filt_b\", \"source1_v_C_filt_c\"]\n",
    "action_to_plot = [\"source1_u_a\"]\n",
    "\n",
    "hook = DataHook(collect_state_ids = states_to_plot)\n",
    "\n",
    "Multi_Agent = setup_agents(env)\n",
    "simulate(Multi_Agent, env, hook=hook)\n",
    "\n",
    "plot_hook_results(hook = hook,\n",
    "                  states_to_plot  = states_to_plot)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "3adf8778587a42468e0a1dd980f94dbf",
   "lastKernelId": "dae90304-e703-47ca-a141-939d9235e6e6"
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
