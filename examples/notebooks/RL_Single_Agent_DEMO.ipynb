{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2888dffe",
   "metadata": {},
   "source": [
    "## Train an RL agent\n",
    "This notebook will focus the following topics:\n",
    "\n",
    " - ### Define reward function,\n",
    " - ### Define featurize function,\n",
    " - ### Training a single RL agent.\n",
    "\n",
    "In the following a reinforcement learning (RL) agent is trained to control the current flowing through an inductor.\n",
    "It will be shown for an easy case how the agent can learn and be applied to an electrical power grid simulated with de JEG package.\n",
    "\n",
    "The use case is shown in the figure below.\n",
    "\n",
    "![](figures/RL_single_agent.png \"\")\n",
    "\n",
    "First we define the environment with the configuration shown in the figure. \n",
    "It consists of a single phase electrical power grid with 1 source and 1 load connected via a cable.\n",
    "For more information on how to setup an environment see `Env_Create_DEMO.ipynb`.\n",
    "\n",
    "`RL` is selected as `control_type` for the source (`parameters[\"source\"][1][\"control_type\"]`).\n",
    "Initially, any key can be used as the `mode`. Here, we choose the name `my_ddpg`. \n",
    "This key is then used to link an agent to the source and its corresponding `state_ids` and `action_ids`.\n",
    "Based on these indices, the state that will be provided to the agent as well as the actions the agent outputs are passed to the appropriate places with the help of a `MultiController`.\n",
    "For further details please refer to Userguide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e01153",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Julia 1.8.2' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "using JEG\n",
    "using ReinforcementLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate passive load for wanted setting / power rating\n",
    "R_load, L_load, X, Z = ParallelLoadImpedance(100e3, 1, 230)\n",
    "\n",
    "# define grid using CM\n",
    "CM = [0. 1.\n",
    "    -1. 0.]\n",
    "\n",
    "# Set parameters accoring graphic above\n",
    "parameters = Dict{Any, Any}(\n",
    "    \"source\" => Any[\n",
    "                    Dict{Any, Any}(\"pwr\" => 200e3, \"control_type\" => \"RL\", \"mode\" => \"my_ddpg\", \"fltr\" => \"L\"),\n",
    "                    ],\n",
    "    \"load\"   => Any[\n",
    "                    Dict{Any, Any}(\"impedance\" => \"R\", \"R\" => R_load, \"v_limit\"=>1e4, \"i_limit\"=>1e4),\n",
    "                    ],\n",
    "    \"grid\" => Dict{Any, Any}(\"phase\" => 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f039119e",
   "metadata": {},
   "source": [
    "To teach the agent that it should control the current in a certain way it needs information about which value the current shoud be (reference value) (->`featurize`) and how good the state is, which was reached using the chosen action (-> `reward`).\n",
    "\n",
    "Therefore, the reference value has to be defined. \n",
    "Here we will use a constant value to keep the example simple.\n",
    "But since the `reference(t)` function take the simulation time as argument, more complex, time dependent signals could be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "function reference(t)\n",
    "    return 1\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55f9ce89",
   "metadata": {},
   "source": [
    "Afterwards the `featurize()` function, which gives the user the opportunity to modify a state before it gets passed to the agent, is defined.\n",
    "\n",
    "It takes three arguments:\n",
    "- `state` contains all the state values that correspond to the source controlled by agent with key `name`\n",
    "- `env` references the environment\n",
    "- `name` contains the key of the agent\n",
    "\n",
    "Then the signal generated by the `reference` function is added to the state for the agent `my_ddpg`. This will help the agent to learn because later we will define a reward that has maximum value if the measured current fits the reference value.\n",
    "The reference value has to be normalized in an appropirate way that it fits to the range of the normalized states.\n",
    "\n",
    "Additionally more signals could be added here to enhance the learning process.\n",
    "\n",
    "As stated before, `state` already contains all state values of the source the agent with key `name` should control.\n",
    "However, the environment maintains a lot more states than that. Through `featurize` we could expose them to the agent but we refrain from that here since we want to simulate a scenario where the source the agent controls is far away (e.g. 1km) from the load its supplying. \n",
    "In cases like this it's common that the agent has no knowlegde about states of the load since no communication and measurements exchange between source and load is assumed.\n",
    "\n",
    "In onther examples the electrical power grid consits of multiple sources and loads. The other sources are controlled by other agents or classic controllers. In that case, typically every controller / agent has knowlegde of the states of the source it controls but not about the states another agent/controller controls.\n",
    "(For more information see `MultiController` and `inner_featurize` of the `env`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23420f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_ddpg\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba45180b",
   "metadata": {},
   "source": [
    "Before defining the environment, the `reward()` function has to be defined. It provides a feedback to the agent on how good the chosen action was.\n",
    "First, the state to be controlled is taken from the current environment state values.\n",
    "Since the states are normalized by the limits the electrical components can handle, a value greater than `1` means that the state limit is exceeded typically leading to a system crash.\n",
    "Therefore, first it is checked if the measured state is greater than `1`. In that case a punishment is returned which, here, is chosen to be `r = -1`.\n",
    "\n",
    "In case the controlled state is within the valid state space, the reward is caculated based on the error between the wanted reference value and the measured state value. \n",
    "If these values are the same, meaning the agent perfectly fullfills the control task, a reward of `r = 1` is returned to the agent. ( -> r $\\in$ [-1, 1]).\n",
    "If the measured value differs from the reference, the error - based on the root-mean square error (RMSE) in this example - is substracted from the maximal reward: `r = 1 - RMSE`:\n",
    "\n",
    "$r = 1 - \\sqrt{\\frac{|i_\\mathrm{L,ref} - i_\\mathrm{L1}|}{2}}$\n",
    "\n",
    "To keep the reward in the wanted range, the current difference is devided by 2. (E.g., in worst case, if a reference value equal to the corresponding current limit is chosen $i_\\mathrm{L,ref} = i_\\mathrm{lim}$ and the measured current is the negative current limit $i_\\mathrm{L1} = -i_\\mathrm{lim}$ more then 1 would be substracted without this normaization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5137a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function reward_function(env, name = nothing)\n",
    "    if name == \"my_ddpg\"\n",
    "        index_1 = findfirst(x -> x == \"source1_i_L1\", env.state_ids)\n",
    "        state_to_control = env.state[index_1]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "            r = 1-((abs.(refs/norm_ref - state_to_control)/2).^0.5)\n",
    "            return r \n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70bb0752",
   "metadata": {},
   "source": [
    "Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electircal power grid. To keep the first learning example simple the action given to the env is internally not delayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ElectricGridEnv(\n",
    "    CM = CM, \n",
    "    parameters = parameters, \n",
    "    t_end = 0.1, \n",
    "    featurize = featurize_ddpg, \n",
    "    reward_function = reward_function, \n",
    "    action_delay = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475ded2d",
   "metadata": {},
   "source": [
    "In this example a `Deep Deterministic Policy Gradient` agent (https://arxiv.org/abs/1509.02971, https://spinningup.openai.com/en/latest/algorithms/ddpg.html) is chosen which can learn a control task on continous state and action spaces.\n",
    "It is configured using the `CreateAgentDdpg()` function which uses the information about the state and action ids, based on the parameter dict, stored in the `agent_dict` in the env:\n",
    "\n",
    "`env.agent_dict[chosen_key]` (chosen key, here, `my_ddpg`):\n",
    "- `\"source_number\"`: ID/number of the source the agent with this key controls\n",
    "- `\"mode\"`: Name of the agent\n",
    "- `\"action_ids\"`: List of strings with the action ids the agent controls/belong to the \"source_number\"`\n",
    "- `\"state_ids\"`: List of strings with the state ids the agent controls/belong to the \"source_number\"`\n",
    "\n",
    "This information is used in the `SetupAgents()` method to configure the control-side of the experiment.\n",
    "\n",
    "The agent is configured to receive as many inputs as environment returns for it's state (after `featurize`) and return as many outputs as actions requested from the env corresponding to the ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CreateAgentDdpg(na = length(env.agent_dict[\"my_ddpg\"][\"action_ids\"]),\n",
    "                          ns = length(state(env, \"my_ddpg\")),\n",
    "                          use_gpu = false)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f0fbaa2",
   "metadata": {},
   "source": [
    "The `SetupAgents()` function takes the control types defined in the parameter dict and hands the correct indices to the corrensponding controllers / agents.\n",
    "The function returns `controllers` which is an instance of the `MultiController` which contains the different agents and classic controllers and maps their actions to the corresponding sources. \n",
    "\n",
    "Since in this example only one RL agent will be used it only contains the defined `my_ddpg` agent. \n",
    "Therefore, the agent handed over to the `SetupAgents()` function is internally extended by a name to a [named policy](https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.NamedPolicy ).\n",
    "Using this name the `MultiController` (compare, https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.MADDPGManager) enables to call the different agents/controllers via name during training and application.\n",
    "\n",
    "To use the previously defined agent, a dict linking tha `chosen_key`: `my_ddpg` to the defined RL agent is handed over to the `SetupAgents` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_agents = Dict(\"my_ddpg\" => agent)\n",
    "\n",
    "controllers = SetupAgents(env, my_custom_agents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cab250b",
   "metadata": {},
   "source": [
    "The `controllers` in this examples consits only of the one RL agent (`my_ddpg`) and can be trained usin the `Learn()` function to train 20 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ff2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learn(controllers, env, num_episodes = 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56598c5",
   "metadata": {},
   "source": [
    "After the training, the `Simulate()` function is used to run a test epiode without action noise and the state to be controlled ($i_\\mathrm{L1}$) is plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states_to_plot = [\"source1_i_L1\"]\n",
    "hook = DataHook(collect_state_ids = states_to_plot)\n",
    "\n",
    "Simulate(controllers, env, hook=hook)\n",
    "\n",
    "RenderHookResults(hook = hook,\n",
    "                  states_to_plot  = states_to_plot)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "3adf8778587a42468e0a1dd980f94dbf",
   "lastKernelId": "dae90304-e703-47ca-a141-939d9235e6e6"
  },
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
