{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example using TD3 to learn a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates how to use TD3 to learn a policy for a simple `ElectricGridEnv`. The environment is the same as the one used in [RL_Complex_DEMO_external_agent](RL_Complex_DEMO_external_agent.ipynb). Furthermore, we introduce how to use `Wandb.jl` to log the training process. For more information on how to customize logging in `Wandb.jl`, please refer to the [documentation](https://avik-pal.github.io/Wandb.jl/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: vikasc-nitk (electricgrid-jl). Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.15.5\n",
      "wandb: Run data is saved locally in /data/cvikas/Projects/ElectricGrid.jl/examples/notebooks/wandb/run-20230714_090659-gmesrzpw\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run train with TD3\n",
      "wandb: â­ï¸ View project at https://wandb.ai/electricgrid-jl/TD3\n",
      "wandb: ğŸš€ View run at https://wandb.ai/electricgrid-jl/TD3/runs/gmesrzpw\n",
      "â”Œ Warning: There is an ongoing wandb run. Please `close` the run before initializing a new one.\n",
      "â”” @ Wandb /upb/users/c/cvikas/profiles/unix/cs/.julia/packages/Wandb/JKy1e/src/main.jl:37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Base.CoreLogging.SimpleLogger(VSCodeServer.IJuliaCore.IJuliaStdio{Base.PipeEndpoint, typeof(VSCodeServer.io_send_callback)}(IOContext(Base.PipeEndpoint(RawFD(21) open, 0 bytes waiting)), VSCodeServer.io_send_callback), Info, Dict{Any, Int64}())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using ElectricGrid\n",
    "using ReinforcementLearning\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using StableRNGs\n",
    "using IntervalSets\n",
    "using Zygote: ignore\n",
    "using Logging\n",
    "using Wandb\n",
    "\n",
    "td3_src_dir = joinpath(dirname(pathof(ElectricGrid)))\n",
    "include(td3_src_dir * \"/agent_td3.jl\")\n",
    "\n",
    "\n",
    "# without using wandb\n",
    "# one using just for wandb\n",
    "# make sure to have a wandb account and be logged in\n",
    "# https://docs.wandb.ai/quickstart\n",
    "logger = WandbLogger(\n",
    "    # Provide a project name and an entity name\n",
    "    project=\"TD3\",\n",
    "    # optionally provide a team name if it is created\n",
    "    entity=\"electricgrid-jl\",\n",
    "    # optionally provide a run name\n",
    "    name=\"train with TD3\",\n",
    "    # optionally provide a config\n",
    "    config=Dict(\n",
    "        \"lr\" => 3e-5,\n",
    "        ),\n",
    ")\n",
    "\n",
    "# override the global logger so that you can easily log\n",
    "# using `@info` \n",
    "# e.g. @info \"metrics\" actor_loss=loss  \n",
    "global_logger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using the same environment configuration \n",
    "CM = [ \n",
    "    0. 0. 1.\n",
    "    0. 0. 2.\n",
    "   -1. -2. 0.\n",
    "]\n",
    "\n",
    "R_load, L_load, _, _ = ParallelLoadImpedance(50e3, 0.95, 230)\n",
    "\n",
    "parameters = Dict{Any, Any}(\n",
    "                    \"source\" => Any[\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"control_type\" => \"RL\",\n",
    "                                        \"mode\" => \"my_agent\",\n",
    "                                        \"fltr\" => \"L\",\n",
    "                                        #\"L1\" => 0.0008,\n",
    "                                        ),\n",
    "                                    Dict{Any, Any}(\n",
    "                                        \"pwr\" => 200e3,\n",
    "                                        \"fltr\" => \"LC\",\n",
    "                                        \"control_type\" => \"classic\",\n",
    "                                        \"mode\" => \"Droop\",),\n",
    "                                    ],\n",
    "                    \"grid\" => Dict{Any, Any}(\n",
    "                        \"phase\" => 3,\n",
    "                        \"ramp_end\" => 0.04,)\n",
    "    )\n",
    "\n",
    "\n",
    "function reference(t)\n",
    "    if t < 0.04\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    end\n",
    "\n",
    "    Î¸ = 2*pi*50*t\n",
    "    Î¸ph = [Î¸; Î¸ - 120Ï€/180; Î¸ + 120Ï€/180]\n",
    "    return +10 * cos.(Î¸ph) \n",
    "end\n",
    "\n",
    "\n",
    "featurize_ddpg = function(state, env, name)\n",
    "    if name == \"my_agent\"\n",
    "        norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]\n",
    "        state = vcat(state, reference(env.t)/norm_ref)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function reward_function(env, name = nothing)\n",
    "    if name == \"classic\"\n",
    "        return 0        \n",
    "    else\n",
    "        state_to_control_1 = env.state[findfirst(x -> x == \"source1_i_L1_a\", env.state_ids)]\n",
    "        state_to_control_2 = env.state[findfirst(x -> x == \"source1_i_L1_b\", env.state_ids)]\n",
    "        state_to_control_3 = env.state[findfirst(x -> x == \"source1_i_L1_c\", env.state_ids)]\n",
    "\n",
    "        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]\n",
    "\n",
    "        if any(abs.(state_to_control).>1)\n",
    "            return -1\n",
    "        else\n",
    "\n",
    "            refs = reference(env.t)\n",
    "            norm_ref = env.nc.parameters[\"source\"][1][\"i_limit\"]          \n",
    "            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))\n",
    "            return r \n",
    "        end\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "env = ElectricGridEnv(\n",
    "    #CM =  CM,\n",
    "    parameters = parameters,\n",
    "    t_end = 1,\n",
    "    reward_function = reward_function,\n",
    "    featurize = featurize_ddpg,\n",
    "    action_delay = 0,\n",
    "    verbosity = 0\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have slightly modified the original TD3 implementation in Julia to make it compatible with the `ElectricGrid.jl` framework.  \n",
    "\n",
    "Details about the TD3 algorithm can be found in the paper [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) and the original implementation in Julia can be found [here](https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl/blob/2e1de3e5b6b8224f50b3d11bba7e1d2d72c6ef7c/src/ReinforcementLearningZoo/src/algorithms/policy_gradient/td3.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = StableRNG(0)\n",
    "init = glorot_uniform(rng)\n",
    "\n",
    "# specify number of states and actions to be controlled by the agent\n",
    "ns = length(ElectricGrid.state(env, \"my_agent\"))\n",
    "na = length(env.agent_dict[\"my_agent\"][\"action_ids\"])\n",
    "\n",
    "CreateActor() = Chain(\n",
    "    Dense(ns, 32, relu; init = init),\n",
    "    Dense(32, 32, relu; init = init),\n",
    "    Dense(32, na, tanh; init = init)\n",
    ")\n",
    "\n",
    "CreateCriticModel() = Chain(\n",
    "    Dense(ns + na, 64, relu; init = init),\n",
    "    Dense(64, 64, relu; init = init),\n",
    "    Dense(64, 1; init = init)\n",
    ")\n",
    "\n",
    "\n",
    "# create twin critic models\n",
    "CreateCritic() = TD3Critic(\n",
    "    CreateCriticModel(),\n",
    "    CreateCriticModel(),\n",
    ")\n",
    "\n",
    "# learning_rate = logger.config[\"lr\"]\n",
    "learning_rate = 3e-5\n",
    "\n",
    "TD3_agent = Agent(\n",
    "    policy = TD3Policy(\n",
    "        behavior_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        behavior_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_actor = NeuralNetworkApproximator(\n",
    "            model = CreateActor(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        target_critic = NeuralNetworkApproximator(\n",
    "            model = CreateCritic(),\n",
    "            optimizer = ADAM(learning_rate),\n",
    "        ),\n",
    "        Î³ = 0.99f0,\n",
    "        Ï = 0.995f0,\n",
    "        batch_size = 64,\n",
    "        start_steps = 10,\n",
    "        # start_steps = -1,\n",
    "        start_policy = RandomPolicy(-1.0..1.0; rng = rng),\n",
    "        update_after = 10,\n",
    "        update_freq = 1,\n",
    "        policy_freq = 2,\n",
    "        target_act_limit = 1.0,\n",
    "        target_act_noise = 0.1,\n",
    "        act_limit = 1.0,\n",
    "        act_noise = 0.05,\n",
    "        rng = rng,\n",
    "    ),\n",
    "\n",
    "    trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 10_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Float32 => (na, ),\n",
    "    ),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Losses and NNlib export \"ctc_loss\"; uses of it in module Flux must be qualified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:   2%|â–‰                                        |  ETA: 0:42:12\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    |  ETA: 0:07:15\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                   |  ETA: 0:06:15\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  |  ETA: 0:05:30\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 |  ETA: 0:04:54\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                |  ETA: 0:04:26\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                |  ETA: 0:04:02\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               |  ETA: 0:03:41\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              |  ETA: 0:03:24\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             |  ETA: 0:03:09\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            |  ETA: 0:02:55\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        |  ETA: 0:01:54\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                |  ETA: 0:00:49\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       |  ETA: 0:00:16\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mProgress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| Time: 0:01:19\u001b[39m\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                 â €â €â €â €â €â €â €â €â €\u001b[97;1mTotal reward per episode\u001b[0mâ €â €â €â €â €â €â €â €â €         \n",
       "                 \u001b[38;5;8mâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\u001b[0m         \n",
       "         \u001b[38;5;8m175.056\u001b[0m \u001b[38;5;8mâ”‚\u001b[0m\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;2mclassic\u001b[0m \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0m\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;4mmy_agent\u001b[0m\n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0m\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0m\u001b[38;5;4mâ ¸\u001b[0m\u001b[38;5;4mâ¡€\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €\u001b[38;5;4mâ¡‡\u001b[0m\u001b[38;5;4mâ¢°\u001b[0m\u001b[38;5;4mâ ¤\u001b[0m\u001b[38;5;4mâ£€\u001b[0m\u001b[38;5;4mâ¢„\u001b[0m\u001b[38;5;4mâ£€\u001b[0m\u001b[38;5;4mâ£€\u001b[0m\u001b[38;5;4mâ£€\u001b[0m\u001b[38;5;4mâ£ \u001b[0mâ €\u001b[38;5;4mâ¡„\u001b[0m\u001b[38;5;4mâ¡€\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €\u001b[38;5;4mâ¢‡\u001b[0m\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €\u001b[38;5;4mâ ‰\u001b[0m\u001b[38;5;4mâ ˆ\u001b[0m\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €\u001b[38;5;4mâ¢¸\u001b[0m\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "   Score        \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €\u001b[38;5;4mâ ˜\u001b[0m\u001b[38;5;4mâ¡¾\u001b[0mâ €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¡‡\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¢£\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                \u001b[38;5;8m\u001b[0m \u001b[38;5;8mâ”‚\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;4mâ¢¸\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "               \u001b[38;5;8m0\u001b[0m \u001b[38;5;8mâ”‚\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;2mâ£€\u001b[0m\u001b[38;5;6mâ£˜\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£¦\u001b[0m\u001b[38;5;6mâ£’\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¢\u001b[0m\u001b[38;5;6mâ£¦\u001b[0m\u001b[38;5;6mâ£”\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¢\u001b[0m\u001b[38;5;6mâ£¦\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£”\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;6mâ£¤\u001b[0m\u001b[38;5;8mâ”‚\u001b[0m \u001b[38;5;8m\u001b[0m        \n",
       "                 \u001b[38;5;8mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\u001b[0m         \n",
       "                 â €\u001b[38;5;8m0\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8m\u001b[0mâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\u001b[38;5;8m100\u001b[0mâ €         \n",
       "                 â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €Episodeâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "td3_agent = Dict(\"my_agent\" => TD3_agent)\n",
    "\n",
    "controllers = SetupAgents(env, td3_agent)\n",
    "\n",
    "Learn(\n",
    "    controllers,\n",
    "    env, \n",
    "    num_episodes = 100,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: \\ 0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\rwandb: \n",
      "wandb: Run history:\n",
      "wandb:       Episode/episode â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "wandb:    metrics/actor_loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–â–â–\n",
      "wandb:   metrics/critic_loss â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–„â–â–†â–â–â–ˆ\n",
      "wandb: total_timesteps/tstep â–ˆâ–†â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:       Episode/episode 1\n",
      "wandb:    metrics/actor_loss -25.94836\n",
      "wandb:   metrics/critic_loss 1.19431\n",
      "wandb: total_timesteps/tstep 14\n",
      "wandb: \n",
      "wandb: ğŸš€ View run train with TD3 at: https://wandb.ai/electricgrid-jl/TD3/runs/qme67myo\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20230713_173906-qme67myo/logs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Python: None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# close wandb session\n",
    "close(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the training metrics on wandb website in realtime. Here we provide the plots of the actor and critic loss during training for illustration purposes:\n",
    "<!-- put images side by side-->\n",
    "<!-- ![Wandb metrics](figures/TD3_actor_loss.svg) -->\n",
    "<img src=\"figures/TD3_actor_loss.svg\"\n",
    "        style=\"width:700px;\"/>\n",
    "<img src=\"figures/TD3_critic_loss.svg\"\n",
    "        style=\"width:700px;\"/>\n",
    "<!-- ![Wandb metrics](figures/TD3_critic_loss.svg) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = DataHook(collect_state_ids = env.state_ids,\n",
    "                collect_action_ids = env.action_ids)\n",
    "\n",
    "hook = Simulate(controllers, env, hook=hook)\n",
    "\n",
    "\n",
    "RenderHookResults(hook = hook,\n",
    "                    states_to_plot  = env.state_ids,\n",
    "                    actions_to_plot = env.action_ids,\n",
    "                    plot_reward=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
