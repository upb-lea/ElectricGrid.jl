<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Multicontroller · ElectricGrid.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ElectricGrid.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ElectricGrid.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><span class="tocitem">Environment</span><ul><li><a class="tocitem" href="../Env_Create/">Configuring the Environment</a></li><li><a class="tocitem" href="../Env_Interaction/">Interaction with the Environment</a></li></ul></li><li><span class="tocitem">Classical Controllers</span><ul><li><a class="tocitem" href="../Classical_Controllers_Swing/">Swing Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_PQ/">PQ Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_Droop/">Droop Controllers</a></li><li><a class="tocitem" href="../Classical_Controllers_VSG/">Virtual Synchronous Generator</a></li><li><a class="tocitem" href="../Auxiliaries_OU_process/">Auxiliaries</a></li></ul></li><li><span class="tocitem">Reinforcement Learning</span><ul><li><a class="tocitem" href="../RL_Single_Agent/">Reinforcement Learning using ElectricGrid</a></li><li class="is-active"><a class="tocitem" href>Multicontroller</a><ul class="internal"><li><a class="tocitem" href="#Train-an-RL-agent-interacting-with-a-stable-grid"><span>Train an RL agent interacting with a stable grid</span></a></li><li><a class="tocitem" href="#Merging-classic-controllers-and-RL-agents"><span>Merging classic controllers and RL agents</span></a></li><li><a class="tocitem" href="#Reward-and-featurize-functions-with-named-policies"><span>Reward and featurize functions with named policies</span></a></li><li><a class="tocitem" href="#MultiController"><span>MultiController</span></a></li><li><a class="tocitem" href="#Training-an-RL-agent-in-a-classicly-controlled-grid"><span>Training an RL agent in a classicly controlled grid</span></a></li></ul></li><li><a class="tocitem" href="../RL_Complex/">Reinforcement Learning in Larger Grids </a></li></ul></li><li><span class="tocitem">Nodeconstructor</span><ul><li><a class="tocitem" href="../NodeConstructor_Theory/">The Nodeconstructor - Theory</a></li><li><a class="tocitem" href="../NodeConstructor_Application/">The Nodeconstructor - Application</a></li></ul></li><li><span class="tocitem">Miscellanous</span><ul><li><a class="tocitem" href="../Default_Parameters/">Default Parameters</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li><li><a class="tocitem" href="../dev/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reinforcement Learning</a></li><li class="is-active"><a href>Multicontroller</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Multicontroller</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/docs/src/RL_Classical_Controllers_Merge.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Train-an-RL-agent-interacting-with-a-stable-grid"><a class="docs-heading-anchor" href="#Train-an-RL-agent-interacting-with-a-stable-grid">Train an RL agent interacting with a stable grid</a><a id="Train-an-RL-agent-interacting-with-a-stable-grid-1"></a><a class="docs-heading-anchor-permalink" href="#Train-an-RL-agent-interacting-with-a-stable-grid" title="Permalink"></a></h2><p>This section will show how combine classic stat-of-the-art controllers with reinforcement learning (RL) agents. As example will show how to control one source with an RL agent learning a control task while connected to a stable grid provided by a classic controller. The topics covered are:</p><ul><li><h3>Merging classic controllers and RL agents,</h3></li><li><h3>Reward and featurize functions with named policys,</h3></li><li><h3>MultiController,</h3></li><li><h3>Training an RL agent in a classicly controlled grid.</h3></li></ul><p>The interactive content related to the section described here can be found in the form of a notebook <a href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/examples/notebooks/RL_Classical_Controllers_Merge_DEMO.ipynb">here</a>.</p><h2 id="Merging-classic-controllers-and-RL-agents"><a class="docs-heading-anchor" href="#Merging-classic-controllers-and-RL-agents">Merging classic controllers and RL agents</a><a id="Merging-classic-controllers-and-RL-agents-1"></a><a class="docs-heading-anchor-permalink" href="#Merging-classic-controllers-and-RL-agents" title="Permalink"></a></h2><p>In the following the RL agent is trained to draw current from a stable grid. The 3-phase electric power grid is formed by a classic controller in open-loop (<code>Swing</code>) mode.  For more details about how the classic control works, see <code>Classical_Controllers_Introduction.ipynb</code>.</p><p>The use case is shown in the figure below. This environment consists of a 3-phase electrical power grid with 2 sources connected via a cable (for improved clarity, only one phase is shown in the following figure).</p><p><img src="../assets/RL_classic_swing.png" alt/></p><p>The first source is controlled by the RL agent <code>my_ddpg</code> which should learn to draw power from the grid, therefore act like an active load. The second source is controlled by a classic controller in open-loop mode.  The swing mode is used to create a stable 3-phase grid to supply the load.</p><p>The environment is configured like described in <a href="https://upb-lea.github.io/ElectricGrid.jl/dev/Env_Create/">Configuring the Environment</a> using the parameter dict:</p><pre><code class="language-julia hljs">using ElectricGrid

parameters = 
Dict{Any, Any}(
    &quot;source&quot; =&gt; Any[
                    Dict{Any, Any}(
                        &quot;pwr&quot; =&gt; 200e3, 
                        &quot;control_type&quot; =&gt; &quot;RL&quot;, 
                        &quot;mode&quot; =&gt; &quot;my_ddpg&quot;, 
                        &quot;fltr&quot; =&gt; &quot;L&quot;),
                    Dict{Any, Any}(
                        &quot;pwr&quot; =&gt; 200e3, 
                        &quot;fltr&quot; =&gt; &quot;LC&quot;, 
                        &quot;control_type&quot; =&gt; 
                        &quot;classic&quot;, &quot;mode&quot; =&gt; 1),
                    ],
    &quot;grid&quot; =&gt; Dict{Any, Any}(
        &quot;phase&quot; =&gt; 3, 
        &quot;ramp_end&quot; =&gt; 0.04,)
)</code></pre><p>An appropriate <code>reference(t)</code> function has to be defined to represent the control objectives. In this example, the time <code>t</code> will be handed over to the function to generate time-varying reference signals. In more detail a three-phase sinusoidal reference signal shifted by 120° and an amplitude of 10 A is created. This should teach the agent to draw time-varying current from the grid.  The phase is thereby chosen similar to the definition in the <code>Swing</code> mode:</p><p class="math-container">\[i_\mathrm{L,ref} =  - 10 \,\text{cos}\left(2 \pi \,50 \, t - \frac{2}{3} \pi (n-1) \right)\]</p><p>, with <span>$n \in [0,1,2]$</span>.</p><p>Here, <span>$n$</span> represents the index refering to the 3 phases of the grid.</p><p>For more enhanced reference functions, the reference current could be chosen with regards to power (active and reactive) reference values. Feel free to implement, change and contribute! </p><pre><code class="language-julia hljs">function reference(t)
    θ = 2*pi*50*t
    θph = [θ; θ - 120π/180; θ + 120π/180]
    return -10 * cos.(θph) 
end</code></pre><h2 id="Reward-and-featurize-functions-with-named-policies"><a class="docs-heading-anchor" href="#Reward-and-featurize-functions-with-named-policies">Reward and featurize functions with named policies</a><a id="Reward-and-featurize-functions-with-named-policies-1"></a><a class="docs-heading-anchor-permalink" href="#Reward-and-featurize-functions-with-named-policies" title="Permalink"></a></h2><p>Afterwards the <code>featurize()</code> function adds the signal generated by the <code>reference</code> function to the state for the agent <code>my_ddpg</code>:</p><pre><code class="language-julia hljs">featurize_ddpg = function(state, env, name)
    if name == &quot;my_ddpg&quot;
        norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]
        state = vcat(state, reference(env.t)/norm_ref)
    end
end</code></pre><p>Then, the <code>reward()</code> function is defined. Here, again it is based on the root-mean square error (RMSE) teaching the agent <code>my_ddpg</code> to match the reference signal to the measured signal. </p><p>If the measured state is greater than <code>1</code> a punishment is returned which is chosen to be <code>r = -1</code>. It not and if the measured value differs from the reference, the average error is substracted from the maximal reward: <code>r = 1 - RMSE</code>:</p><p class="math-container">\[r = 1 - \frac{1}{3} \sum_{{p \in \{\mathrm{a,b,c}\}}} \sqrt{\frac{|i_\mathrm{L,ref,p} - i_\mathrm{L1,p}|}{2}}\]</p><p>This function is only used if the name of the policy is <code>my_ddpg</code> which was chosen in the parameter dict.  In any other case, 1 is returned. This could be used to define 2 different reward functions for 2 different agents via name (e.g., <code>my_ddpg</code> and <code>my_sac</code>) to learn for example a current control with the <code>my_ddpg</code> agent but a voltage control task with the <code>my_sac</code> agent.</p><p>Here, in any case but <code>name == my_ddpg</code> - so in case of the <code>classic</code> controller <code>r = 1</code> is returned.</p><pre><code class="language-julia hljs">function reward_function(env, name = nothing)
    if name == &quot;my_ddpg&quot;
        state_to_control_1 = env.state[findfirst(x -&gt; x == &quot;source1_i_L1_a&quot;, env.state_ids)]
        state_to_control_2 = env.state[findfirst(x -&gt; x == &quot;source1_i_L1_b&quot;, env.state_ids)]
        state_to_control_3 = env.state[findfirst(x -&gt; x == &quot;source1_i_L1_c&quot;, env.state_ids)]

        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]

        if any(abs.(state_to_control).&gt;1)
            return -1
        else

            refs = reference(env.t)
            norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]          
            r = 1-1/3*(sum((abs.(refs/norm_ref - state_to_control)/2).^0.5))
            return r 
        end
    else
        return 1
    end

end</code></pre><p>Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electircal power grid.  Here, no <code>CM</code> matrix defining the connection is used. Since the grid consists only of 2 sources that is the only connection possible.  The <code>ElectricGridEnv</code> creates this internally based on the length of the parameter dict sources.</p><pre><code class="language-julia hljs">env = ElectricGridEnv(
    parameters = parameters, 
    t_end = 0.1, 
    featurize = featurize_ddpg, 
    reward_function = reward_function, 
    action_delay = 0);</code></pre><h2 id="MultiController"><a class="docs-heading-anchor" href="#MultiController">MultiController</a><a id="MultiController-1"></a><a class="docs-heading-anchor-permalink" href="#MultiController" title="Permalink"></a></h2><p>The <code>MultiController</code> ensured that the correct states and actions are linked to the corresponding controllers/agents based on the <code>&quot;control_type&quot;</code> and <code>&quot;mode&quot;</code> defined in the parameter dict.</p><ul><li><p><code>&quot;control_type&quot; = &quot;classic&quot;</code>: Predefined classic controllers are used to calculate the actions for that source based on its states.</p></li><li><p><code>&quot;control_type&quot; = &quot;RL&quot;</code>: Based on the defined agent name (here, <code>my_ddpg</code>), the corresponding states are forwarded to the defined RL agent which returns the actions beloning to the source.</p></li></ul><p>In the following, we will use the <code>CreateAgentDDPG()</code> methode to create the DDPG <code>agent</code>. This <code>agent</code> is linked in the <code>my_custom_agents</code> dict to the chosen name <code>my_ddpg</code>, which is used in the <code>SetupAgents</code> method to configure the control side of the experiment:</p><pre><code class="language-julia hljs">agent = CreateAgentDdpg(na = length(env.agent_dict[&quot;my_ddpg&quot;][&quot;action_ids&quot;]),
                          ns = length(state(env, &quot;my_ddpg&quot;)),
                          use_gpu = false)

my_custom_agents = Dict(&quot;my_ddpg&quot; =&gt; agent)

controllers = SetupAgents(env, my_custom_agents);</code></pre><p>Like shown in the following figure, the <code>controllers</code> struct consists of 2 agents now - one per source.</p><p><img src="../assets/Overview_example.png" alt/></p><p>Since 2 sources are defined in the env here, one controlled classically and the other by RL, the <code>MultiController</code> hands over the correct indices of the environment to the controllers. This enables each controller, e.g., to find the correct subset of states in the entire environment state set. In the parameter dict the first source is labeled to be controlled by the RL agent we named <code>my_ddpg</code>: </p><pre><code class="language-julia hljs">controllers.agents[&quot;my_ddpg&quot;]</code></pre><pre><code class="nohighlight hljs">Dict{Any, Any} with 3 entries:
  &quot;policy&quot;     =&gt; typename(Agent)…
  &quot;action_ids&quot; =&gt; [&quot;source1_u_a&quot;, &quot;source1_u_b&quot;, &quot;source1_u_c&quot;]
  &quot;state_ids&quot;  =&gt; [&quot;source1_i_L1_a&quot;, &quot;source1_v_C_cables_a&quot;, &quot;source1_i_L1_b&quot;, …</code></pre><p>Like introduced, it has knowledge about the state and action indices of the first source.</p><p>The second source is controlled via the classic controller: </p><pre><code class="language-julia hljs">controllers.agents[&quot;classic&quot;]</code></pre><pre><code class="nohighlight hljs">Dict{Any, Any} with 3 entries:
  &quot;policy&quot;     =&gt; typename(NamedPolicy)…
  &quot;action_ids&quot; =&gt; [&quot;source2_u_a&quot;, &quot;source2_u_b&quot;, &quot;source2_u_c&quot;]
  &quot;state_ids&quot;  =&gt; [&quot;source2_i_L1_a&quot;, &quot;source2_v_C_filt_a&quot;, &quot;source2_v_C_cables_…</code></pre><h2 id="Training-an-RL-agent-in-a-classicly-controlled-grid"><a class="docs-heading-anchor" href="#Training-an-RL-agent-in-a-classicly-controlled-grid">Training an RL agent in a classicly controlled grid</a><a id="Training-an-RL-agent-in-a-classicly-controlled-grid-1"></a><a class="docs-heading-anchor-permalink" href="#Training-an-RL-agent-in-a-classicly-controlled-grid" title="Permalink"></a></h2><p>Then the <code>Learn()</code> function can be used to train the agent.  Here, only the RL agent is trained.  The classic controller is executed to control the second source without parameter adaptions. The <code>Simulate()</code> function can be used to run a test episode without action noise.</p><pre><code class="language-julia hljs">hook_learn = Learn(controller, env, num_episodes = 1000);
hook_sim = Simulate(controller, env, hook=hook);</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../RL_Single_Agent/">« Reinforcement Learning using ElectricGrid</a><a class="docs-footer-nextpage" href="../RL_Complex/">Reinforcement Learning in Larger Grids  »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 2 July 2023 05:37">Sunday 2 July 2023</span>. Using Julia version 1.9.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
