<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reinforcement Learning using ElectricGrid · ElectricGrid.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ElectricGrid.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ElectricGrid.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><span class="tocitem">Environment</span><ul><li><a class="tocitem" href="../Env_Create/">Configuring the Environment</a></li><li><a class="tocitem" href="../Env_Interaction/">Interaction with the Environment</a></li></ul></li><li><span class="tocitem">Classical Controllers</span><ul><li><a class="tocitem" href="../Classical_Controllers_Swing/">Swing Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_PQ/">PQ Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_Droop/">Droop Controllers</a></li><li><a class="tocitem" href="../Classical_Controllers_VSG/">Virtual Synchronous Generator</a></li><li><a class="tocitem" href="../Auxiliaries_OU_process/">Auxiliaries</a></li></ul></li><li><span class="tocitem">Reinforcement Learning</span><ul><li class="is-active"><a class="tocitem" href>Reinforcement Learning using ElectricGrid</a><ul class="internal"><li><a class="tocitem" href="#Introduction-to-RL-with-ElectricGrid.jl"><span>Introduction to RL with ElectricGrid.jl</span></a></li><li><a class="tocitem" href="#Define-featurize-function"><span>Define featurize function</span></a></li><li><a class="tocitem" href="#Define-reward-function"><span>Define reward function</span></a></li><li><a class="tocitem" href="#Training-a-single-RL-agent"><span>Training a single RL agent</span></a></li></ul></li><li><a class="tocitem" href="../RL_Classical_Controllers_Merge/">Multicontroller</a></li><li><a class="tocitem" href="../RL_Complex/">Reinforcement Learning in Larger Grids </a></li></ul></li><li><span class="tocitem">Nodeconstructor</span><ul><li><a class="tocitem" href="../NodeConstructor_Theory/">The Nodeconstructor - Theory</a></li><li><a class="tocitem" href="../NodeConstructor_Application/">The Nodeconstructor - Application</a></li></ul></li><li><span class="tocitem">Miscellaneous</span><ul><li><a class="tocitem" href="../Default_Parameters/">Default Parameters</a></li><li><a class="tocitem" href="../Nonlinear/">Nonlinear Components</a></li><li><a class="tocitem" href="../DC_link_models/">DC Link Models</a></li><li><a class="tocitem" href="../Gui/">GUI</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li><li><a class="tocitem" href="../dev/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reinforcement Learning</a></li><li class="is-active"><a href>Reinforcement Learning using ElectricGrid</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reinforcement Learning using ElectricGrid</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/docs/src/RL_Single_Agent.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Train-an-RL-agent"><a class="docs-heading-anchor" href="#Train-an-RL-agent">Train an RL agent</a><a id="Train-an-RL-agent-1"></a><a class="docs-heading-anchor-permalink" href="#Train-an-RL-agent" title="Permalink"></a></h1><p>This section will introduce how to train a reinforcement learning agent using the ElectricGrid.jl framework. The content is as follows:</p><ul><li><h3>Introduction to RL with ElectricGrid.jl,</h3></li><li><h3>Define featurize function,</h3></li><li><h3>Define reward function,</h3></li><li><h3>Training a single RL agent.</h3></li></ul><p>The interactive content related to the section described here can be found in the form of a notebook <a href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/examples/notebooks/RL_Single_Agent_DEMO.ipynb">here</a>.</p><h2 id="Introduction-to-RL-with-ElectricGrid.jl"><a class="docs-heading-anchor" href="#Introduction-to-RL-with-ElectricGrid.jl">Introduction to RL with ElectricGrid.jl</a><a id="Introduction-to-RL-with-ElectricGrid.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-to-RL-with-ElectricGrid.jl" title="Permalink"></a></h2><p>In the following, a reinforcement learning (RL) agent is trained to control the current flowing through an inductor. It will be shown how the agent can learn and be applied to an simple electrical power grid simulated with de <code>ElectricGrid</code> package.</p><p>The use case is shown in the figure below.</p><p><img src="../assets/RL_single_agent.png" alt/></p><p>First we define the environment with the configuration shown in the figure.  It consists of a single phase electrical power grid with 1 source and 1 load connected via a cable. For more information on how to setup an environment see <code>Env_Create_DEMO.ipynb</code>.</p><p>The parameter dict is used for configuration. Within the source dict, two imporant entires will have to be defined:</p><ul><li><code>&quot;control_type&quot;</code>: Defines whether the source is controlled applying a classic controller (<code>&quot;classic&quot;</code>) or using an RL agent (<code>&quot;RL&quot;</code>).</li><li><code>&quot;mode&quot;</code>: Defines the control mode of controller/agent:<ul><li><code>&quot;classic&quot;</code>: Different predefined classic control modes can be chosen to apply a specific control behaviour (e.g., droop, VSG, step response,...). For more information see Classic_Controllers-notebooks or UserGuide.</li><li><code>&quot;RL&quot;</code>: An agent name, which is used later on to link the defined agent to this source.</li></ul></li></ul><p>Here, <code>RL</code> is selected as <code>control_type</code> and the name <code>my_ddpg</code> as the <code>mode</code>. </p><pre><code class="language-julia hljs">using ElectricGrid</code></pre><pre><code class="language-julia hljs"># calculate passive load for some setting / power rating
R_load, L_load, X, Z = ParallelLoadImpedance(100e3, 1, 230)

# define grid using CM
CM = [0. 1.
    -1. 0.]

# Set parameters according to graphic above
parameters = Dict{Any, Any}(
    &quot;source&quot; =&gt; Any[
                    Dict{Any, Any}(&quot;pwr&quot; =&gt; 200e3, &quot;control_type&quot; =&gt; &quot;RL&quot;, &quot;mode&quot; =&gt; &quot;my_ddpg&quot;, &quot;fltr&quot; =&gt; &quot;L&quot;),
                    ],
    &quot;load&quot;   =&gt; Any[
                    Dict{Any, Any}(&quot;impedance&quot; =&gt; &quot;R&quot;, &quot;R&quot; =&gt; R_load, &quot;v_limit&quot;=&gt;1e4, &quot;i_limit&quot;=&gt;1e4),
                    ],
    &quot;grid&quot; =&gt; Dict{Any, Any}(&quot;phase&quot; =&gt; 1)
);</code></pre><p>In the following, the agent <code>my_ddpg</code> should learn to control the current of the source. Therefore, we define a reference value. The goal of the agent is to reach and track this reference signal. Using the <code>reward()</code> function, the agent receives feedback on the current state and the selected action.</p><p>Therefore, the reference value has to be defined.  Here, we will use a constant value to keep the example simple. But since the <code>reference(t)</code> function takes the simulation time as argument, more complex, time-dependent signals could be defined.</p><p>The reference for this example is to output <span>$i_\mathrm{L1} = 10\,\mathrm{A}$</span>:</p><pre><code class="language-julia hljs">function reference(t)
    return 10.0
end</code></pre><pre><code class="nohighlight hljs">reference (generic function with 1 method)</code></pre><h2 id="Define-featurize-function"><a class="docs-heading-anchor" href="#Define-featurize-function">Define featurize function</a><a id="Define-featurize-function-1"></a><a class="docs-heading-anchor-permalink" href="#Define-featurize-function" title="Permalink"></a></h2><p>Afterwards the <code>featurize()</code> function, which gives the user the opportunity to modify the measured states before they get passed to the agent, is defined.</p><p>It takes three arguments:</p><ul><li><code>state</code> contains all the state values that correspond to the source controlled by agent,</li><li><code>env</code> references the environment,</li><li><code>name</code> contains the key of the agent.</li></ul><p>In the following, the signal generated by the <code>reference</code> function is added to the state provided to the agent <code>my_ddpg</code>. This will help the agent to learn because later we will define a reward that has maximum value if the measured current fits the reference value. The reference value has to be normalized in an appropirate way that it fits to the range of the normalized states.</p><p>Additionally, more signals could be added here to enhance the learning process.</p><p>As stated before, <code>state</code> already contains all state values of the source the agent with key <code>name</code> should control. However, the environment maintains a lot more states than that. Through <code>featurize</code> we could expose them to the agent but we refrain from that here since we want to simulate a scenario where the source the agent controls is far away (e.g. 1km) from the load its supplying.  In cases like this it&#39;s common that the agent has no knowlegde about states of the load since no communication and measurements exchange between source and load is assumed.</p><p>In anonther example, the electrical power grid consits of multiple sources and loads. The other sources are controlled by other agents or classic controllers. In that case, typically every controller/agent has knowlegde of the states of the source it controls but not about the states another agent/controller controls. Handing the corresponding source-depending states to the correct controllers/agents is handeled internally based on the <code>keys</code> (above: <code>my_ddpg</code>- for more information see <code>MultiController</code> and <code>inner_featurize</code> of the <code>env</code>).</p><pre><code class="language-julia hljs">featurize_ddpg = function(state, env, name)
    if name == &quot;my_ddpg&quot;
        norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]
        state = vcat(state, reference(env.t)/norm_ref)
    end
end</code></pre><pre><code class="nohighlight hljs">#11 (generic function with 1 method)</code></pre><h2 id="Define-reward-function"><a class="docs-heading-anchor" href="#Define-reward-function">Define reward function</a><a id="Define-reward-function-1"></a><a class="docs-heading-anchor-permalink" href="#Define-reward-function" title="Permalink"></a></h2><p>The <code>reward()</code> function provides a feedback to the agent on how good the chosen action was. First, the state to be controlled is taken from the current environment state values. Since the states are normalized by the limits the electrical components can handle, a value greater than <code>1</code> means that the state limit is exceeded typically leading to a system crash. Therefore, first it is checked if the measured state is greater than <code>1</code>. In that case a punishment is returned which, here, is chosen to be <code>r = -1</code>.</p><p>In case the controlled state is within the valid state space, the reward is caculated based on the error between the wanted reference value and the measured state value.  If these values are the same, meaning the agent perfectly fullfills the control task, a reward of <code>r = 1</code> is returned to the agent. ( -&gt; r <span>$\in$</span> [-1, 1]). If the measured value differs from the reference, the error - based on the root-mean square error (RMSE) in this example - is substracted from the maximal reward: <code>r = 1 - RMSE</code>:</p><p class="math-container">\[r = 1 - \sqrt{\frac{|i_\mathrm{L,ref} - i_\mathrm{L1}|}{2}}\]</p><p>To keep the reward in the wanted range, the current difference is devided by 2. </p><pre><code class="language-julia hljs">function reward_function(env, name = nothing)
    index_1 = findfirst(x -&gt; x == &quot;source1_i_L1&quot;, env.state_ids)
    state_to_control = env.state[index_1]

    if any(abs.(state_to_control).&gt;1)
        return -1
    else

        refs = reference(env.t)
        norm_ref = env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;]          
        r = 1-((abs.(refs/norm_ref - state_to_control)/2).^0.5)
        return r 
    end
end</code></pre><pre><code class="nohighlight hljs">reward_function (generic function with 2 methods)</code></pre><h2 id="Training-a-single-RL-agent"><a class="docs-heading-anchor" href="#Training-a-single-RL-agent">Training a single RL agent</a><a id="Training-a-single-RL-agent-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-single-RL-agent" title="Permalink"></a></h2><p>Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electrical power grid. To keep the first learning example simple, the action given to the env is internally not delayed (as it would be in a real-world digitally controlled system). </p><pre><code class="language-julia hljs">env = ElectricGridEnv(
    CM = CM, 
    parameters = parameters, 
    t_end = 0.025, 
    featurize = featurize_ddpg, 
    reward_function = reward_function, 
    action_delay = 0);</code></pre><pre><code class="nohighlight hljs">******************************************************************************
This program contains Ipopt, a library for large-scale nonlinear optimization.
 Ipopt is released as open source code under the Eclipse Public License (EPL).
         For more information visit https://github.com/coin-or/Ipopt
******************************************************************************</code></pre><p>In this example, a <code>Deep Deterministic Policy Gradient</code> agent (compare the corresponding <a href="https://arxiv.org/abs/1509.02971">paper</a>, or this nice <a href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html">introdution</a>) is chosen which can learn a control task on continous state and action spaces. It is configured using the <code>CreateAgentDdpg()</code> function:</p><pre><code class="language-julia hljs">agent = CreateAgentDdpg(na = length(env.agent_dict[&quot;my_ddpg&quot;][&quot;action_ids&quot;]),
                          ns = length(state(env, &quot;my_ddpg&quot;)),
                          use_gpu = false);</code></pre><p>The environment stores the indices (states and actions) to which the agent has access based on the definition in the parameter dict. This can be found in the  <code>agent_dict</code> using the defined agent name.</p><p><code>env.agent_dict[chosen_name]</code> (chosen name, here, <code>my_ddpg</code>):</p><ul><li><code>&quot;source_number&quot;</code>: ID/number of the source the agent with this key controls</li><li><code>&quot;mode&quot;</code>: Name of the agent</li><li><code>&quot;action_ids&quot;</code>: List of strings with the action ids the agent controls/belong to the <code>&quot;source_number&quot;</code></li><li><code>&quot;state_ids&quot;</code>: List of strings with the state ids the agent controls/belong to the <code>&quot;source_number&quot;</code></li></ul><p>After defining the agent, the control side of the experiment is configured in the <code>SetupAgents()</code> method. The function returns <code>controller</code> which is an instance of the <code>MultiController</code> that contains the different agents and classic controllers and maps their actions to the corresponding sources.</p><pre><code class="language-julia hljs">my_custom_agents = Dict(&quot;my_ddpg&quot; =&gt; agent)

controller = SetupAgents(env, my_custom_agents);</code></pre><pre><code class="nohighlight hljs">WARNING: both JumpProcesses and DelayDiffEq export &quot;first_tstop&quot;; uses of it in module DifferentialEquations must be qualified
WARNING: both ControlSystemsBase and DifferentialEquations export &quot;isdiscrete&quot;; uses of it in module ElectricGrid must be qualified
WARNING: both JumpProcesses and DelayDiffEq export &quot;pop_tstop!&quot;; uses of it in module DifferentialEquations must be qualified
WARNING: both JumpProcesses and DelayDiffEq export &quot;has_tstop&quot;; uses of it in module DifferentialEquations must be qualified
WARNING: both Losses and NNlib export &quot;ctc_loss&quot;; uses of it in module Flux must be qualified</code></pre><p>To use the predefined agent in the experiment, the <code>SetupAgents()</code> takes a dictonary as secound input. Like shown above, the dictonary <code>my_custom_agents</code> links the predefined <code>agent</code> to the chosen agent name (<code>&quot;mode&quot;</code>) in the parameter dict.</p><p>Internally, the <code>SetupAgents()</code> function extends the <code>agent</code> by a name to a <a href="https://juliareinforcementlearning.org/docs/rlcore/#ReinforcementLearningCore.NamedPolicy ">named policy</a>. Using this named policy the <a href="https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.MADDPGManager">MultiController</a> enables to call the different agents/controllers via name during training and application. This is used for example in the <code>reward()</code> function to return a specific reward to the <code>my_ddpg</code> agent (<code>if name == &quot;my_ddpg&quot; ...</code>) Later, that functionality helps to give different rewards to different agents seperated by name.</p><p>Now, the agent can be trained for 30000 steps using the <code>Learn()</code>:</p><pre><code class="language-julia hljs">learnhook = Learn(controller, env, steps = 30_000);</code></pre><pre><code class="nohighlight hljs">Progress: 100%|█████████████████████████████████████████| Time: 0:00:46[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         225.245 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ my_ddpg
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.873 │⠀⢱⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀200⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀</code></pre><p>The learning curve of accumulated reward per episode is printed to the command line/output window as shown above.</p><p>To fine-tune the results we want to further train the agent with decreasing action noise levels. To do this, we introduce a learning function that implements an action noise scheduler.</p><pre><code class="language-julia hljs">function learn_ans()
    num_steps = 10_000

    an_scheduler_loops = 7
    
    an = 0.001 * exp10.(collect(LinRange(0.0, -13, an_scheduler_loops)))

    for i in 1:an_scheduler_loops
        controller.agents[&quot;my_ddpg&quot;][&quot;policy&quot;].policy.policy.act_noise = an[i]
        println(&quot;Steps so far: $(length(controller.hook.df[!,&quot;reward&quot;]))&quot;)
        println(&quot;next action noise level: $(an[i])&quot;)
        Learn(controller, env, steps = num_steps, hook = learnhook)
    end
end</code></pre><pre><code class="nohighlight hljs">learn_ans (generic function with 1 method)</code></pre><pre><code class="language-julia hljs">learn_ans()</code></pre><pre><code class="nohighlight hljs">Steps so far: 30000
next action noise level: 0.001


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         242.042 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡜⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠸⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⠀⢣⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀200⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 40000
next action noise level: 6.812920690579615e-6


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         246.819 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡰⠻⠟⠋⠋⠉⠉⠁│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠁⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡎⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⠈⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠃⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⠀⢣⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀200⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 50000
next action noise level: 4.641588833612782e-8


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         247.786 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣶⠞⠛⠋⠋⠋⠛⠉⠻⠁⠀⠀⠀⠀⠀⠀⠀⠀│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⠘⣄⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀300⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 60000
next action noise level: 3.162277660168379e-10


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         248.119 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⠞⠞⠋⠛⠋⠛⠉⠻⠉⠓⠞⠙⠛⠉⠀⠀⠀│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⠘⣄⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀300⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 70000
next action noise level: 2.1544346900318868e-12


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         248.119 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠞⠛⠛⠛⠹⠋⠳⠛⠛⠉⠛⠋⠋⠃⠀⠀⠀⠀⠀⠀⠀⠀│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⢸⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀400⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 80000
next action noise level: 1.4677992676220677e-14


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         248.119 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠞⠛⠛⠛⠹⠋⠳⠛⠛⠉⠛⠋⠋⠛⠛⠋⠙⠁⠀⠀⠀⠀│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⢸⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀400⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀        


Steps so far: 90000
next action noise level: 1.0000000000000001e-16


Progress: 100%|█████████████████████████████████████████| Time: 0:00:02[K



                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀Total reward per episode⠀⠀⠀⠀⠀⠀⠀⠀⠀        
                 ┌────────────────────────────────────────┐        
         248.358 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⠞⠻⠛⠛⠹⠛⠳⠛⠛⠉⠛⠛⠛⠛⠛⠛⠙⠛⠛⠛⠙⠁│ my_ddpg
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
   Score         │⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 │⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
         144.394 │⢸⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│        
                 └────────────────────────────────────────┘        
                 ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀400⠀        
                 ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Episode⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀</code></pre><p>After the training, the <code>Simulate()</code> function is used to run a test epiode without action noise and afterwards the input voltage (<span>$v_{i1}$</span>) and current (<span>$i_\mathrm{L1}$</span>) of the source is plotted:</p><pre><code class="language-julia hljs">states_to_plot = [&quot;source1_i_L1&quot;]
actions_to_plot = [&quot;source1_u&quot;]

hook = DataHook(collect_state_ids = states_to_plot, collect_action_ids = actions_to_plot)

Simulate(controller, env, hook=hook)

RenderHookResults(hook = hook,
                  states_to_plot  = states_to_plot,
                  actions_to_plot = actions_to_plot)</code></pre><div
id = ZvGpqSu4vIRy > </div>
<script>
gd = 'ZvGpqSu4vIRy'
require(['plotly'], function(plotly) {

    plotly.newPlot(gd,[{"mode":"lines","y":[0.0,6.889623302443561,11.564306508904348,13.982253296027766,14.619957988766744,14.232008451448706,13.551852183031802,13.059876848766262,12.89659174389596,12.92613826571868,12.893518763734171,12.59169295609006,11.967137035801429,11.132163448363734,10.297651636817712,9.670822897404596,9.369032832571342,9.38423239251667,9.604861384393931,9.876346520798688,10.068404050727509,10.120130238406212,10.04834736915965,9.92262487436124,9.823633965227767,9.805330036961145,9.876016423904069,10.002618938489677,10.131960342981149,10.216847268057613,10.235028910189392,10.194301826798965,10.12411014579168,10.059610765044553,10.026247875320626,10.031194944168611,10.063977671566331,10.104320738753929,10.1325976514428,10.138027506504937,10.12160607519402,10.093524290225801,10.067184726145669,10.052962627680213,10.054382353492556,10.06787187288138,10.085540764327165,10.099260006088056,10.104092768322378,10.099757266646925,10.089871413960712,10.079705446017176,10.073660258223313,10.073579208983952,10.078446866457767,10.085344172400317,10.091026641821557,10.093347973406257,10.091960067085028,10.088129221078038,10.083908742775712,10.081133889359386,10.080693569251268,10.082333219886062,10.08497347027113,10.087315678391484,10.088427379440208,10.08806581335244,10.086652246768013,10.084972415388336,10.083779382428002,10.083483468442864,10.084043259978628,10.085066282589413,10.086038376979634,10.086561777876213,10.086500410225572,10.085988586808288,10.085324689091951,10.084815676316893,10.084646731876564,10.084826083622907,10.08521425580275,10.085609881182041,10.085845624111055,10.085852105173064,10.085668921269665,10.08540791134514,10.08519290006251,10.085105542954954,10.085158864489326,10.085304474416354,10.085464213802464,10.08556844226443,10.085583678055485,10.085519962955713,10.085418608107723,10.085329085998296,10.085286767743064,10.085300688910733,10.085354623569124,10.0854185574493,10.085463826034644,10.085474889973417,10.085453429137992,10.085414456854748,10.085377558561989,10.085357831017841,10.085360378423523,10.085380017266154,10.085405351446605,10.085424700788373,10.085431053487321,10.085424145231203,10.085409323597657,10.08539426800479,10.08538533737864,10.085385146491737,10.085392160734834,10.085402106438966,10.085410269709286,10.085413555254629,10.085411485858703,10.08540591891241,10.085399835931016,10.085395884116526,10.085395327596157,10.085397772616249,10.085401638996107,10.085405042201053,10.085406640142486,10.085406095982691,10.085404033733157,10.085401598973089,10.085399881918768,10.08539947029875,10.08540029562677,10.085401783160115,10.08540318653294,10.085403932567075,10.085403829729927,10.085403077979443,10.085402112557475,10.085401377937135,10.085401139658105,10.08540140598664,10.08540197197593,10.085402544831037,10.085402882967104,10.08540288789535,10.085402619104473,10.08540223992036,10.085401929807151,10.08540180609949,10.085401886257532,10.085402098992876,10.085402330569686,10.085402480359285,10.085402500633162,10.08540240681274,10.085402259333822,10.085402129977519,10.085402069661718,10.085402090934906,10.085402169800266,10.085402262533815,10.085402327671241,10.085402342984985,10.085402311255155,10.08540225448332,10.085402201111838,10.085402172902224,10.085402177040251,10.085402205814159,10.085402242601928,10.0854022704904,10.085402279424194,10.08540226915719,10.08540224754356,10.085402225746517,10.085402212943942,10.085402212844995,10.085402223144339,10.085402237600377,10.085402249380799,10.085402254037945,10.085402250934042,10.085402242804953,10.085402233989221,10.085402228311972,10.085402227575024,10.085402231174882,10.085402236800228,10.085402241716993,10.085402243993464,10.08540224316241,10.085402240146536,10.085402236614645,10.085402234143714,10.085402233574328,10.085402234794042,10.085402236960675,10.085402238990373,10.085402240056965,10.085402239890938,10.085402238789637,10.08540223738786,10.085402236329156,10.08540223599429,10.085402236390022,10.085402237215366,10.085402238044718,10.085402238529422,10.085402238529813,10.085402238135188,10.085402237584088,10.085402237136588,10.08540223696129,10.085402237081452,10.085402237392064,10.085402237727655,10.085402237942827,10.085402237969555,10.085402237831445,10.085402237616883,10.085402237429998,10.085402237344077,10.085402237376519,10.08540223749185,10.085402237626374,10.0854022377201,10.085402237741256,10.085402237694376,10.085402237611698,10.085402237534513,10.085402237494181,10.085402237500812,10.085402237542963,10.085402237596377,10.085402237636567,10.08540223764911,10.085402237633865,10.085402237602349,10.085402237570792,10.085402237552445,10.08540223755256,10.085402237567681,10.085402237588688,10.085402237605695,10.085402237612282,10.085402237607626],"type":"scatter","name":"source1_i_L1","x":[0.0,0.0001,0.0002,0.00030000000000000003,0.0004,0.0005,0.0006000000000000001,0.0007000000000000001,0.0008000000000000001,0.0009000000000000002,0.0010000000000000002,0.0011000000000000003,0.0012000000000000003,0.0013000000000000004,0.0014000000000000004,0.0015000000000000005,0.0016000000000000005,0.0017000000000000006,0.0018000000000000006,0.0019000000000000006,0.0020000000000000005,0.0021000000000000003,0.0022,0.0023,0.0024,0.0024999999999999996,0.0025999999999999994,0.0026999999999999993,0.002799999999999999,0.002899999999999999,0.0029999999999999988,0.0030999999999999986,0.0031999999999999984,0.0032999999999999982,0.003399999999999998,0.003499999999999998,0.0035999999999999977,0.0036999999999999976,0.0037999999999999974,0.0038999999999999972,0.0039999999999999975,0.004099999999999998,0.004199999999999998,0.004299999999999998,0.0043999999999999985,0.004499999999999999,0.004599999999999999,0.004699999999999999,0.0048,0.0049,0.005,0.0051,0.005200000000000001,0.005300000000000001,0.005400000000000001,0.005500000000000001,0.005600000000000002,0.005700000000000002,0.005800000000000002,0.0059000000000000025,0.006000000000000003,0.006100000000000003,0.006200000000000003,0.0063000000000000035,0.006400000000000004,0.006500000000000004,0.006600000000000004,0.0067000000000000046,0.006800000000000005,0.006900000000000005,0.007000000000000005,0.007100000000000006,0.007200000000000006,0.007300000000000006,0.007400000000000006,0.007500000000000007,0.007600000000000007,0.007700000000000007,0.0078000000000000074,0.007900000000000008,0.008000000000000007,0.008100000000000007,0.008200000000000006,0.008300000000000005,0.008400000000000005,0.008500000000000004,0.008600000000000003,0.008700000000000003,0.008800000000000002,0.008900000000000002,0.009000000000000001,0.0091,0.0092,0.0093,0.009399999999999999,0.009499999999999998,0.009599999999999997,0.009699999999999997,0.009799999999999996,0.009899999999999996,0.009999999999999995,0.010099999999999994,0.010199999999999994,0.010299999999999993,0.010399999999999993,0.010499999999999992,0.010599999999999991,0.01069999999999999,0.01079999999999999,0.01089999999999999,0.010999999999999989,0.011099999999999988,0.011199999999999988,0.011299999999999987,0.011399999999999987,0.011499999999999986,0.011599999999999985,0.011699999999999985,0.011799999999999984,0.011899999999999984,0.011999999999999983,0.012099999999999982,0.012199999999999982,0.012299999999999981,0.01239999999999998,0.01249999999999998,0.01259999999999998,0.012699999999999979,0.012799999999999978,0.012899999999999977,0.012999999999999977,0.013099999999999976,0.013199999999999976,0.013299999999999975,0.013399999999999974,0.013499999999999974,0.013599999999999973,0.013699999999999973,0.013799999999999972,0.013899999999999971,0.01399999999999997,0.01409999999999997,0.01419999999999997,0.014299999999999969,0.014399999999999968,0.014499999999999968,0.014599999999999967,0.014699999999999967,0.014799999999999966,0.014899999999999965,0.014999999999999965,0.015099999999999964,0.015199999999999964,0.015299999999999963,0.015399999999999962,0.015499999999999962,0.015599999999999961,0.01569999999999996,0.01579999999999996,0.01589999999999996,0.01599999999999996,0.016099999999999958,0.016199999999999957,0.016299999999999957,0.016399999999999956,0.016499999999999956,0.016599999999999955,0.016699999999999954,0.016799999999999954,0.016899999999999953,0.016999999999999953,0.017099999999999952,0.01719999999999995,0.01729999999999995,0.01739999999999995,0.01749999999999995,0.01759999999999995,0.01769999999999995,0.017799999999999948,0.017899999999999947,0.017999999999999947,0.018099999999999946,0.018199999999999945,0.018299999999999945,0.018399999999999944,0.018499999999999944,0.018599999999999943,0.018699999999999942,0.018799999999999942,0.01889999999999994,0.01899999999999994,0.01909999999999994,0.01919999999999994,0.01929999999999994,0.019399999999999938,0.019499999999999938,0.019599999999999937,0.019699999999999936,0.019799999999999936,0.019899999999999935,0.019999999999999934,0.020099999999999934,0.020199999999999933,0.020299999999999933,0.020399999999999932,0.02049999999999993,0.02059999999999993,0.02069999999999993,0.02079999999999993,0.02089999999999993,0.02099999999999993,0.021099999999999928,0.021199999999999927,0.021299999999999927,0.021399999999999926,0.021499999999999925,0.021599999999999925,0.021699999999999924,0.021799999999999924,0.021899999999999923,0.021999999999999922,0.022099999999999922,0.02219999999999992,0.02229999999999992,0.02239999999999992,0.02249999999999992,0.02259999999999992,0.022699999999999918,0.022799999999999918,0.022899999999999917,0.022999999999999916,0.023099999999999916,0.023199999999999915,0.023299999999999915,0.023399999999999914,0.023499999999999913,0.023599999999999913,0.023699999999999912,0.02379999999999991,0.02389999999999991,0.02399999999999991,0.02409999999999991,0.02419999999999991,0.02429999999999991,0.024399999999999908,0.024499999999999907,0.024599999999999907,0.024699999999999906,0.024799999999999905,0.024899999999999905,0.024999999999999904]},{"mode":"lines","y":[24.350418802955026,20.324610660472153,17.657812204711835,16.33812007708741,16.049742614638728,16.327675463165452,16.738910600726864,17.014999419634314,17.093580168956397,17.070985285962998,17.10351684390315,17.308493662674522,17.70623039042538,18.218996869969274,18.71590055577403,19.075306370020932,19.234362273429404,19.206398477950746,19.06415265416811,18.901603223639608,18.79393583338301,18.772501278729816,18.822354663613638,18.899174234505644,18.954870938968817,18.959571216686545,18.91148700716164,18.832851557216824,18.756278948799594,18.709108971402273,18.702739908304984,18.73055696722701,18.77377889747688,18.811335088081364,18.82890246075429,18.82347688776218,18.80239125244378,18.778217662511327,18.76244867051858,18.760828266614173,18.771980776535884,18.789297865690653,18.804684580185118,18.812244618284517,18.81036628443398,18.80162678963064,18.790956129376983,18.783150995029494,18.78090100576442,18.784060446136085,18.79024108501874,18.796233641673783,18.79950889742958,18.799154472451452,18.795959009621782,18.791766373403963,18.788505970820776,18.787366075572887,18.788430737843306,18.79084710218164,18.793355040427297,18.79489001803996,18.794992765620723,18.793889575478307,18.792270361223277,18.790913385214818,18.790342371805778,18.790653832856588,18.791555570057362,18.792561165032527,18.79323003034001,18.793343887022445,18.792957370527084,18.79232478354826,18.791756601213933,18.79147890255101,18.79155415839797,18.79188527335746,18.792285808756766,18.792574704139177,18.79265123767872,18.79252243201079,18.792280195553047,18.792047075809222,18.791919180995187,18.79193073849252,18.792051161340222,18.792209848085232,18.792333188860677,18.79237599831086,18.792335218434133,18.792243418127903,18.79214854851853,18.792090985726134,18.79208799272114,18.7921307479134,18.79219286535687,18.792244709282073,18.79226642744035,18.79225447234825,18.792220077751068,18.792181815617727,18.792156449615174,18.792152236808707,18.792167035305535,18.792191123876076,18.792212680236823,18.792223128676532,18.79222013780765,18.792207444984182,18.792192167283144,18.792181191439468,18.792178327333215,18.792183278104073,18.792192523336627,18.79220139139724,18.792206231881348,18.79220575831996,18.79220115115449,18.79219510680472,18.792190426448645,18.79218882156074,18.792190397250952,18.792193904929675,18.792197516181673,18.792199696916832,18.792199796559412,18.792198157752303,18.792195789131696,18.792193819164314,18.792193000610837,18.792193464145083,18.79219477845797,18.792196235013773,18.792197196499153,18.79219735102277,18.79219678283762,18.792195863813504,18.792195044301174,18.792194649708875,18.79219476707287,18.792195252581497,18.792195834551073,18.792196251022215,18.7921963578566,18.792196167438394,18.792195814563133,18.79219547727626,18.792195294192105,18.792195313628536,18.792195490018624,18.792195720370263,18.79219589808058,18.79219595832371,18.792195897527854,18.792195763557636,18.792195626132457,18.792195543541343,18.792195540301883,18.792195603113807,18.792195693424876,18.79219576826894,18.79219579910573,18.792195781128807,18.79219573089737,18.79219567544247,18.792195638992713,18.79219563332816,18.79219565513783,18.792195690196962,18.792195721350353,18.79219573625048,18.7921957316492,18.792195713079465,18.79219569091192,18.792195675110552,18.792195671128194,18.792195678451947,18.792195691920842,18.792195704749314,18.7921957116741,18.792195710880772,18.792195704128,18.79219569534951,18.792195688602245,18.792195686341405,18.792195688686647,18.792195693803183,18.792195699032725,18.792195702160395,18.792195702262553,18.792195699855277,18.79219569641193,18.792195693568456,18.792195692406914,18.79219569310383,18.79219569502356,18.79219569713483,18.7921956985166,18.792195698723898,18.79219569788682,18.792195696549392,18.792195695365162,18.79219569480261,18.792195694982826,18.792195695693074,18.792195696537398,18.79219569713691,18.79219569728531,18.792195697003645,18.792195696489554,18.792195696001645,18.792195695739768,18.79219569577197,18.792195696030515,18.792195696365006,18.79219569662119,18.79219569670604,18.792195696615536,18.792195696420162,18.792195696221185,18.79219569610274,18.79219569609966,18.792195696191904,18.792195696323166,18.792195696431186,18.79219569647494,18.792195696447955,18.79219569637461,18.792195696294186,18.79219569624183,18.792195696234266,18.792195696266404,18.792195696317414,18.79219569636239,18.792195696383615,18.792195696376606,18.79219569634944,18.792195696317272,18.792195696294506,18.792195696289014,18.792195696299824,18.792195696319443,18.79219569633802,18.792195696347914,18.792195696346617,18.7921956963367,18.792195696323986,18.792195696314202,18.792195696311033,18.792195696314547],"type":"scatter","name":"source1_u","yaxis":"y2","x":[0.0,0.0001,0.0002,0.00030000000000000003,0.0004,0.0005,0.0006000000000000001,0.0007000000000000001,0.0008000000000000001,0.0009000000000000002,0.0010000000000000002,0.0011000000000000003,0.0012000000000000003,0.0013000000000000004,0.0014000000000000004,0.0015000000000000005,0.0016000000000000005,0.0017000000000000006,0.0018000000000000006,0.0019000000000000006,0.0020000000000000005,0.0021000000000000003,0.0022,0.0023,0.0024,0.0024999999999999996,0.0025999999999999994,0.0026999999999999993,0.002799999999999999,0.002899999999999999,0.0029999999999999988,0.0030999999999999986,0.0031999999999999984,0.0032999999999999982,0.003399999999999998,0.003499999999999998,0.0035999999999999977,0.0036999999999999976,0.0037999999999999974,0.0038999999999999972,0.0039999999999999975,0.004099999999999998,0.004199999999999998,0.004299999999999998,0.0043999999999999985,0.004499999999999999,0.004599999999999999,0.004699999999999999,0.0048,0.0049,0.005,0.0051,0.005200000000000001,0.005300000000000001,0.005400000000000001,0.005500000000000001,0.005600000000000002,0.005700000000000002,0.005800000000000002,0.0059000000000000025,0.006000000000000003,0.006100000000000003,0.006200000000000003,0.0063000000000000035,0.006400000000000004,0.006500000000000004,0.006600000000000004,0.0067000000000000046,0.006800000000000005,0.006900000000000005,0.007000000000000005,0.007100000000000006,0.007200000000000006,0.007300000000000006,0.007400000000000006,0.007500000000000007,0.007600000000000007,0.007700000000000007,0.0078000000000000074,0.007900000000000008,0.008000000000000007,0.008100000000000007,0.008200000000000006,0.008300000000000005,0.008400000000000005,0.008500000000000004,0.008600000000000003,0.008700000000000003,0.008800000000000002,0.008900000000000002,0.009000000000000001,0.0091,0.0092,0.0093,0.009399999999999999,0.009499999999999998,0.009599999999999997,0.009699999999999997,0.009799999999999996,0.009899999999999996,0.009999999999999995,0.010099999999999994,0.010199999999999994,0.010299999999999993,0.010399999999999993,0.010499999999999992,0.010599999999999991,0.01069999999999999,0.01079999999999999,0.01089999999999999,0.010999999999999989,0.011099999999999988,0.011199999999999988,0.011299999999999987,0.011399999999999987,0.011499999999999986,0.011599999999999985,0.011699999999999985,0.011799999999999984,0.011899999999999984,0.011999999999999983,0.012099999999999982,0.012199999999999982,0.012299999999999981,0.01239999999999998,0.01249999999999998,0.01259999999999998,0.012699999999999979,0.012799999999999978,0.012899999999999977,0.012999999999999977,0.013099999999999976,0.013199999999999976,0.013299999999999975,0.013399999999999974,0.013499999999999974,0.013599999999999973,0.013699999999999973,0.013799999999999972,0.013899999999999971,0.01399999999999997,0.01409999999999997,0.01419999999999997,0.014299999999999969,0.014399999999999968,0.014499999999999968,0.014599999999999967,0.014699999999999967,0.014799999999999966,0.014899999999999965,0.014999999999999965,0.015099999999999964,0.015199999999999964,0.015299999999999963,0.015399999999999962,0.015499999999999962,0.015599999999999961,0.01569999999999996,0.01579999999999996,0.01589999999999996,0.01599999999999996,0.016099999999999958,0.016199999999999957,0.016299999999999957,0.016399999999999956,0.016499999999999956,0.016599999999999955,0.016699999999999954,0.016799999999999954,0.016899999999999953,0.016999999999999953,0.017099999999999952,0.01719999999999995,0.01729999999999995,0.01739999999999995,0.01749999999999995,0.01759999999999995,0.01769999999999995,0.017799999999999948,0.017899999999999947,0.017999999999999947,0.018099999999999946,0.018199999999999945,0.018299999999999945,0.018399999999999944,0.018499999999999944,0.018599999999999943,0.018699999999999942,0.018799999999999942,0.01889999999999994,0.01899999999999994,0.01909999999999994,0.01919999999999994,0.01929999999999994,0.019399999999999938,0.019499999999999938,0.019599999999999937,0.019699999999999936,0.019799999999999936,0.019899999999999935,0.019999999999999934,0.020099999999999934,0.020199999999999933,0.020299999999999933,0.020399999999999932,0.02049999999999993,0.02059999999999993,0.02069999999999993,0.02079999999999993,0.02089999999999993,0.02099999999999993,0.021099999999999928,0.021199999999999927,0.021299999999999927,0.021399999999999926,0.021499999999999925,0.021599999999999925,0.021699999999999924,0.021799999999999924,0.021899999999999923,0.021999999999999922,0.022099999999999922,0.02219999999999992,0.02229999999999992,0.02239999999999992,0.02249999999999992,0.02259999999999992,0.022699999999999918,0.022799999999999918,0.022899999999999917,0.022999999999999916,0.023099999999999916,0.023199999999999915,0.023299999999999915,0.023399999999999914,0.023499999999999913,0.023599999999999913,0.023699999999999912,0.02379999999999991,0.02389999999999991,0.02399999999999991,0.02409999999999991,0.02419999999999991,0.02429999999999991,0.024399999999999908,0.024499999999999907,0.024599999999999907,0.024699999999999906,0.024799999999999905,0.024899999999999905,0.024999999999999904]}],
        {"xaxis":{"title":{"text":"Time in Seconds"}},"font":{"color":"black","family":"Arial","size":16},"template":{"layout":{"coloraxis":{"colorbar":{"ticks":"","outlinewidth":0}},"xaxis":{"gridcolor":"white","zerolinewidth":2,"title":{"standoff":15},"ticks":"","zerolinecolor":"white","automargin":true,"linecolor":"white"},"hovermode":"closest","paper_bgcolor":"white","geo":{"showlakes":true,"showland":true,"landcolor":"#E5ECF6","bgcolor":"white","subunitcolor":"white","lakecolor":"white"},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"yaxis":{"gridcolor":"white","zerolinewidth":2,"title":{"standoff":15},"ticks":"","zerolinecolor":"white","automargin":true,"linecolor":"white"},"shapedefaults":{"line":{"color":"#2a3f5f"}},"hoverlabel":{"align":"left"},"mapbox":{"style":"light"},"polar":{"angularaxis":{"gridcolor":"white","ticks":"","linecolor":"white"},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","ticks":"","linecolor":"white"}},"autotypenumbers":"strict","font":{"color":"#2a3f5f"},"ternary":{"baxis":{"gridcolor":"white","ticks":"","linecolor":"white"},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","ticks":"","linecolor":"white"},"aaxis":{"gridcolor":"white","ticks":"","linecolor":"white"}},"annotationdefaults":{"arrowhead":0,"arrowwidth":1,"arrowcolor":"#2a3f5f"},"plot_bgcolor":"#E5ECF6","title":{"x":0.05},"scene":{"xaxis":{"gridcolor":"white","gridwidth":2,"backgroundcolor":"#E5ECF6","ticks":"","showbackground":true,"zerolinecolor":"white","linecolor":"white"},"zaxis":{"gridcolor":"white","gridwidth":2,"backgroundcolor":"#E5ECF6","ticks":"","showbackground":true,"zerolinecolor":"white","linecolor":"white"},"yaxis":{"gridcolor":"white","gridwidth":2,"backgroundcolor":"#E5ECF6","ticks":"","showbackground":true,"zerolinecolor":"white","linecolor":"white"}},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"]},"data":{"barpolar":[{"type":"barpolar","marker":{"line":{"color":"#E5ECF6","width":0.5}}}],"carpet":[{"aaxis":{"gridcolor":"white","endlinecolor":"#2a3f5f","minorgridcolor":"white","startlinecolor":"#2a3f5f","linecolor":"white"},"type":"carpet","baxis":{"gridcolor":"white","endlinecolor":"#2a3f5f","minorgridcolor":"white","startlinecolor":"#2a3f5f","linecolor":"white"}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"parcoords":[{"line":{"colorbar":{"ticks":"","outlinewidth":0}},"type":"parcoords"}],"scatter":[{"type":"scatter","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"histogram2dcontour":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"histogram2dcontour","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contour":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"contour","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"mesh3d":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"mesh3d"}],"surface":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"surface","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"histogram":[{"type":"histogram","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"pie":[{"type":"pie","automargin":true}],"choropleth":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"choropleth"}],"heatmapgl":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"heatmapgl","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"bar":[{"type":"bar","error_y":{"color":"#2a3f5f"},"error_x":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5}}}],"heatmap":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"heatmap","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"contourcarpet"}],"table":[{"type":"table","header":{"line":{"color":"white"},"fill":{"color":"#C8D4E3"}},"cells":{"line":{"color":"white"},"fill":{"color":"#EBF0F8"}}}],"scatter3d":[{"line":{"colorbar":{"ticks":"","outlinewidth":0}},"type":"scatter3d","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"histogram2d":[{"colorbar":{"ticks":"","outlinewidth":0},"type":"histogram2d","colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"ticks":"","outlinewidth":0}}}]}},"legend":{"yanchor":"bottom","xanchor":"right","y":1.02,"orientation":"h","x":1},"margin":{"l":100,"b":80,"pad":10,"r":80,"t":100},"plot_bgcolor":"#f1f3f7","yaxis":{"title":{"text":"State values"}},"yaxis2":{"titlefont":{"color":"orange"},"overlaying":"y","title":"Action values","side":"right"}},
        {"editable":false,"responsive":true,"staticPlot":false,"scrollZoom":true},
    )
});


    </script><p>Above, the simulation results show that the RL-based controller is able to roughly meet the constant current reference of 10 A. Further training, RL topology changes or improved observation features can further improve the learned control performance. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Auxiliaries_OU_process/">« Auxiliaries</a><a class="docs-footer-nextpage" href="../RL_Classical_Controllers_Merge/">Multicontroller »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 29 September 2023 08:55">Friday 29 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
