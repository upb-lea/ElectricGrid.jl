<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reinforcement Learning in Larger Grids  · ElectricGrid.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="ElectricGrid.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ElectricGrid.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><span class="tocitem">Environment</span><ul><li><a class="tocitem" href="../Env_Create/">Configuring the Environment</a></li><li><a class="tocitem" href="../Env_Interaction/">Interaction with the Environment</a></li></ul></li><li><span class="tocitem">Classical Controllers</span><ul><li><a class="tocitem" href="../Classical_Controllers_Swing/">Swing Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_PQ/">PQ Mode</a></li><li><a class="tocitem" href="../Classical_Controllers_Droop/">Droop Controllers</a></li><li><a class="tocitem" href="../Classical_Controllers_VSG/">Virtual Synchronous Generator</a></li><li><a class="tocitem" href="../Auxiliaries_OU_process/">Auxiliaries</a></li></ul></li><li><span class="tocitem">Reinforcement Learning</span><ul><li><a class="tocitem" href="../RL_Single_Agent/">Reinforcement Learning using ElectricGrid</a></li><li><a class="tocitem" href="../RL_Classical_Controllers_Merge/">Multicontroller</a></li><li class="is-active"><a class="tocitem" href>Reinforcement Learning in Larger Grids </a><ul class="internal"><li><a class="tocitem" href="#Experiment-one-RL-agent-controlling-different-sources"><span>Experiment one RL agent controlling different sources</span></a></li><li><a class="tocitem" href="#Featurize-and-reward-for-all-three-sources"><span>Featurize and reward for all three sources</span></a></li><li><a class="tocitem" href="#Train-an-agent-to-control-all-three-sources"><span>Train an agent to control all three sources</span></a></li></ul></li></ul></li><li><span class="tocitem">Nodeconstructor</span><ul><li><a class="tocitem" href="../NodeConstructor_Theory/">The Nodeconstructor - Theory</a></li><li><a class="tocitem" href="../NodeConstructor_Application/">The Nodeconstructor - Application</a></li></ul></li><li><span class="tocitem">Miscellanous</span><ul><li><a class="tocitem" href="../Default_Parameters/">Default Parameters</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li><li><a class="tocitem" href="../dev/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reinforcement Learning</a></li><li class="is-active"><a href>Reinforcement Learning in Larger Grids </a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reinforcement Learning in Larger Grids </a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/docs/src/RL_Complex.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Train-an-RL-agent-controlling-different-sources"><a class="docs-heading-anchor" href="#Train-an-RL-agent-controlling-different-sources">Train an RL agent controlling different sources</a><a id="Train-an-RL-agent-controlling-different-sources-1"></a><a class="docs-heading-anchor-permalink" href="#Train-an-RL-agent-controlling-different-sources" title="Permalink"></a></h1><p>In section a reinforcement learning agent is trained control 3 sources.  The following topics will be covered:</p><ul><li><h3>Experiment one RL agent controlling different sources</h3></li><li><h3>Featurize and reward for all three sources</h3></li><li><h3>Train an agent to control all three sources</h3></li></ul><p>The interactive content related to the section described here can be found in the form of a notebook <a href="https://github.com/upb-lea/ElectricGrid.jl/blob/main/examples/notebooks/RL_Complex_DEMO.ipynb">here</a>.</p><h2 id="Experiment-one-RL-agent-controlling-different-sources"><a class="docs-heading-anchor" href="#Experiment-one-RL-agent-controlling-different-sources">Experiment one RL agent controlling different sources</a><a id="Experiment-one-RL-agent-controlling-different-sources-1"></a><a class="docs-heading-anchor-permalink" href="#Experiment-one-RL-agent-controlling-different-sources" title="Permalink"></a></h2><p>The experiment is shown in the figure below. Two sources will be fed by a negative reference value and therefore act as active loads. The electrical power grid is here abstracted similar to the output of the <code>DrawGraph(env.nc)</code> methode. The sources and loads (here no passive loads are available) shown as colored circles connected via cables. This is similar like the usage of the GUI, where the sources, loads and cables can be parameterized and connected interactivly.</p><p><img src="../assets/RL_Complex_Demo.png" alt/></p><p>The environment is configured like described in <a href="https://upb-lea.github.io/ElectricGrid.jl/dev/Env_Create/">Configuring the Environment</a> using the parameter dict. It can be noticed, that the control <code>mode</code> for all three sources is set to the same <code>my_ddpg</code> agent. As <code>reference(t)</code> function for simlicity, DC-values are used, one per source, since we are dealing with a single phase grid. The first an third reference values are negative, so these sources will draw power from the grid. The secound reference value is positiv, so the secound source will provide power.</p><pre><code class="language-julia hljs">using ElectricGrid

CM = [0.0   1.0  0
     -1.0   0.0  2.0
     0  -2.0  0.0]

parameters =
Dict{Any, Any}(
    &quot;source&quot; =&gt; Any[
                    Dict{Any, Any}(
                        &quot;pwr&quot; =&gt; 200e3,
                        &quot;control_type&quot; =&gt; &quot;RL&quot;,
                        &quot;mode&quot; =&gt; &quot;my_ddpg&quot;,
                        &quot;fltr&quot; =&gt; &quot;L&quot;),
                    Dict{Any, Any}(
                        &quot;pwr&quot; =&gt; 200e3,
                        &quot;fltr&quot; =&gt; &quot;LC&quot;,
                        &quot;control_type&quot; =&gt;
                        &quot;RL&quot;, &quot;mode&quot; =&gt; &quot;my_ddpg&quot;),
                    Dict{Any, Any}(
                        &quot;pwr&quot; =&gt; 200e3,
                        &quot;fltr&quot; =&gt; &quot;L&quot;,
                        &quot;control_type&quot; =&gt;
                        &quot;RL&quot;, &quot;mode&quot; =&gt; &quot;my_ddpg&quot;),
                    ],
    &quot;grid&quot; =&gt; Dict{Any, Any}(
        &quot;phase&quot; =&gt; 1,
        &quot;ramp_end&quot; =&gt; 0.04,)
)

function reference(t)
    return [-10., 230., -15.]
end</code></pre><h2 id="Featurize-and-reward-for-all-three-sources"><a class="docs-heading-anchor" href="#Featurize-and-reward-for-all-three-sources">Featurize and reward for all three sources</a><a id="Featurize-and-reward-for-all-three-sources-1"></a><a class="docs-heading-anchor-permalink" href="#Featurize-and-reward-for-all-three-sources" title="Permalink"></a></h2><p>Afterwards the <code>featurize()</code> function adds the signal generated by the <code>reference</code> function to the state for the agent <code>my_ddpg</code>. All reference values are normalized regarding to the limit current or voltage of the source they will refer to (like shown later):</p><pre><code class="language-julia hljs">featurize_ddpg = function(state, env, name)
    if name == &quot;my_ddpg&quot;
        refs = reference(env.t)
        refs[1] = refs[1] / env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;] 
        refs[2] = refs[2] / env.nc.parameters[&quot;source&quot;][2][&quot;v_limit&quot;] 
        refs[3] = refs[3] / env.nc.parameters[&quot;source&quot;][3][&quot;i_limit&quot;] 
        
        state = vcat(state, refs)
    end
end</code></pre><p>The <code>reward()</code> function again it is based on the root-mean square error (RMSE) teach the agent <code>my_ddpg</code> to match the reference signal to the measured signal. </p><p>If the measured state is greater than <code>1</code>. In that case a punishment is returned which, here, is chosen to be <code>r = -1</code>. It not and if the measured value differs from the reference, the average error is substracted from the maximal reward: <code>r = 1 - RMSE</code>:</p><p class="math-container">\[r = 1 - \frac{1}{3} \sum_{{p \in \{\mathrm{a,b,c}\}}} \sqrt{\frac{|x_\mathrm{ref,p} - x_\mathrm{meas,p}|}{2}}\]</p><p><strong>Important here is the choise of the states!</strong></p><p>This is done in the first lines of the <code>reward()</code> function:</p><p>For the first and third source the current thougth the inductors are used.  Since the <code>reference</code> value is negative, the agent should learn to draw current from the grid. For the secound source, the voltage accros the capacitor is used and the reference value is positive.  Therefore, the agent should learn to supply the capacitor / &quot;build up a grid&quot; (while the other 2 sources draw current).</p><pre><code class="language-julia hljs">function reward_function(env, name = nothing)
    if name == &quot;my_ddpg&quot;
        state_to_control_1 = env.state[findfirst(x -&gt; x == &quot;source1_i_L1&quot;, env.state_ids)]
        state_to_control_2 = env.state[findfirst(x -&gt; x == &quot;source2_v_C_filt&quot;, env.state_ids)]
        state_to_control_3 = env.state[findfirst(x -&gt; x == &quot;source3_i_L1&quot;, env.state_ids)]


        state_to_control = [state_to_control_1, state_to_control_2, state_to_control_3]

        if any(abs.(state_to_control).&gt;1)
            return -1
        else

            refs = reference(env.t)
            refs[1] = refs[1] / env.nc.parameters[&quot;source&quot;][1][&quot;i_limit&quot;] 
            refs[2] = refs[2] / env.nc.parameters[&quot;source&quot;][2][&quot;v_limit&quot;] 
            refs[3] = refs[3] / env.nc.parameters[&quot;source&quot;][3][&quot;i_limit&quot;]   

            r = 1-1/3*(sum((abs.(refs - state_to_control)/2).^0.5))
            return r 
        end
    else
        return 1
    end

end</code></pre><h2 id="Train-an-agent-to-control-all-three-sources"><a class="docs-heading-anchor" href="#Train-an-agent-to-control-all-three-sources">Train an agent to control all three sources</a><a id="Train-an-agent-to-control-all-three-sources-1"></a><a class="docs-heading-anchor-permalink" href="#Train-an-agent-to-control-all-three-sources" title="Permalink"></a></h2><p>Then, the defined parameters, featurize and reward functions are used to create an environment consisting of the electrical power grid. Like shown in <code>RL_Single_Agent_DEMO.ipynb</code>, again an DDPG agent is created.  The <code>SetupAgents()</code> function is then used to configure the <code>controllers</code> utilizing the <code>MultiController</code>.</p><pre><code class="language-julia hljs">env = ElectricGridEnv(
    CM = CM,
    parameters = parameters, 
    t_end = 0.1, 
    featurize = featurize_ddpg, 
    reward_function = reward_function, 
    action_delay = 0);

agent = CreateAgentDdpg(na = length(env.agent_dict[&quot;my_ddpg&quot;][&quot;action_ids&quot;]),
                          ns = length(state(env, &quot;my_ddpg&quot;)),
                          use_gpu = false)

my_custom_agents = Dict(&quot;my_ddpg&quot; =&gt; agent)

controllers = SetupAgents(env, my_custom_agents);</code></pre><p>Here, the <code>controllers</code> struct consists only of the one <code>my_ddpg</code> agent. This agent puts out three different actions, one per source:</p><pre><code class="language-julia hljs">controllers.agents[&quot;my_ddpg&quot;][&quot;action_ids&quot;]</code></pre><pre><code class="nohighlight hljs">3-element Vector{Any}:
 &quot;source1_u&quot;
 &quot;source2_u&quot;
 &quot;source3_u&quot;</code></pre><p>And has knowlegde about all the states of the three sources:</p><pre><code class="language-julia hljs">controllers.agents[&quot;my_ddpg&quot;][&quot;state_ids&quot;]</code></pre><pre><code class="nohighlight hljs">7-element Vector{Any}:
 &quot;source1_i_L1&quot;
 &quot;source1_v_C_cables&quot;
 &quot;source2_i_L1&quot;
 &quot;source2_v_C_filt&quot;
 &quot;source2_v_C_cables&quot;
 &quot;source3_i_L1&quot;
 &quot;source3_v_C_cables&quot;</code></pre><p>The next steps are straigth forward compared to the prior RL example notebooks like <code>RL_Classical_Controllers_Merge_DEMO.ipynb</code> to train the agent or simulate a test run:</p><pre><code class="language-julia hljs">hook_learn = Learn(controller, env, num_episodes = 1000);
hook_sim = Simulate(controller, env, hook=hook);</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../RL_Classical_Controllers_Merge/">« Multicontroller</a><a class="docs-footer-nextpage" href="../NodeConstructor_Theory/">The Nodeconstructor - Theory »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Tuesday 1 August 2023 09:28">Tuesday 1 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
